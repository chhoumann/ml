{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Tokenizing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./data/the-verdict.txt', <http.client.HTTPMessage at 0x7fe3dd63a810>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/\"\n",
    "    \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "    \"the-verdict.txt\"\n",
    ")\n",
    "\n",
    "file_path = \"./data/the-verdict.txt\"\n",
    "urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"./data/the-verdict.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    raw_text = file.read()\n",
    "\n",
    "print(f\"Total number of characters: {len(raw_text)}\")\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'Is', ' ', 'this', '--', '', ' ', 'a', ' ', 'test', '?', '']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r\"([,.:;?_!\\\"()']|--|\\s)\", text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "# Should you keep or remove whitespaces?\n",
    "# Depends on the application & its requirements.\n",
    "# Removing them reduces memory & computational requirements.\n",
    "# But they might be important for some applications, like Python code, which is whitespace-sensitive.\n",
    "# We remove it here, but will later switch to a method that keeps whitespaces.\n",
    "\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed = re.split(r\"([,.:;?_!\\\"()']|--|\\s)\", raw_text)\n",
    "preprocessed = [item for item in preprocessed if item.strip()]\n",
    "assert len(preprocessed) == 4690, \"Amount of tokens should be 4690\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Coverting tokens into token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "assert vocab_size == 1130, \"Vocabulary size should be 1130\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token: i for i, token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab: Dict[str, int]):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i: token for token, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        preprocessed = re.split(r\"([,.:;?_!\\\"()']|--|\\s)\", text)\n",
    "        preprocessed = [item for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[token] for token in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, tokens: List[int]) -> str:\n",
    "        text = \" \".join([self.int_to_str[token] for token in tokens])\n",
    "        # Remove whitespaces before punctuation marks\n",
    "        text = re.sub(r\" ([,.:;?_!\\\"()'])\", r\"\\1\", text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"\"It's the last he painted, you know,\" \n",
    "       Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert ids == [\n",
    "    1,\n",
    "    56,\n",
    "    2,\n",
    "    850,\n",
    "    988,\n",
    "    602,\n",
    "    533,\n",
    "    746,\n",
    "    5,\n",
    "    1126,\n",
    "    596,\n",
    "    5,\n",
    "    1,\n",
    "    67,\n",
    "    7,\n",
    "    38,\n",
    "    851,\n",
    "    1108,\n",
    "    754,\n",
    "    793,\n",
    "    7,\n",
    "], \"IDs should be as expected\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError: 'Hello'\n"
     ]
    }
   ],
   "source": [
    "# \"Hello\" is not in the vocabulary, so it will raise a KeyError\n",
    "# We can deal with the kind of error by adding special tokens to the vocabulary.\n",
    "try:\n",
    "    text = \"Hello, do you like tea?\"\n",
    "    print(tokenizer.encode(text))\n",
    "except KeyError as e:\n",
    "    print(f\"KeyError: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Adding special context tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(set(preprocessed))\n",
    "all_tokens.extend([\"<|unk|>\", \"<|endoftext|>\"])\n",
    "vocab = {token: i for i, token in enumerate(all_tokens)}\n",
    "\n",
    "print(len(vocab.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab: Dict[str, int]):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i: token for token, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        preprocessed = re.split(r\"([,.:;?_!\\\"()']|--|\\s)\", text)\n",
    "        preprocessed = [item for item in preprocessed if item.strip()]\n",
    "        # Replace unknown tokens with \"<|unk|>\"\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "        ids = [self.str_to_int[token] for token in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, tokens: List[int]) -> str:\n",
    "        text = \" \".join([self.int_to_str[token] for token in tokens])\n",
    "        # Remove whitespaces before punctuation marks\n",
    "        text = re.sub(r\" ([,.:;?_!\\\"()'])\", r\"\\1\", text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1130, 5, 355, 1126, 628, 975, 10, 1131, 55, 988, 956, 984, 722, 988, 1130, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Byte pair encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.8.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "\n",
    "print(f\"tiktoken version: {version('tiktoken')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\" \"of someunknownPlace\"\n",
    ")\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Data sampling with a sliding window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample = enc_text[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1 : context_size + 1]\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] --> 4920\n",
      "[290, 4920] --> 2241\n",
      "[290, 4920, 2241] --> 287\n",
      "[290, 4920, 2241, 287] --> 257\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size + 1):\n",
    "    context = enc_sample[:i]\n",
    "    target = enc_sample[i]\n",
    "    print(context, \"-->\", target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and -->  established\n",
      " and established -->  himself\n",
      " and established himself -->  in\n",
      " and established himself in -->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size + 1):\n",
    "    context = enc_sample[:i]\n",
    "    target = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"-->\", tokenizer.decode([target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(\n",
    "        self, txt: str, tokenizer: tiktoken.Encoding, max_length: int, stride: int\n",
    "    ):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the text\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "\n",
    "        # Chunk text into overlapping sequences of max_length using the sliding window\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i : i + max_length]\n",
    "            target_chunk = token_ids[i + 1 : i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Total number of samples in the dataset.\"\"\"\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get a sample from the dataset at the given index.\"\"\"\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(\n",
    "    txt: str,\n",
    "    batch_size: int = 4,\n",
    "    max_length: int = 256,\n",
    "    stride: int = 128,\n",
    "    shuffle: bool = True,\n",
    "    drop_last: bool = True,\n",
    "    num_workers: int = 0,\n",
    "):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`batch_size` is the number of samples per batch. Small batch sizes require less memory, but can lead to more noisy model updates.\n",
    "\n",
    "`drop_last` drops the last batch if it's shorter than the specified `batch_size`.\n",
    "This prevents loss spikes during training.\n",
    "\n",
    "`stride` is the step size for the sliding window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n",
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "with open(\"./data/the-verdict.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    raw_text = file.read()\n",
    "\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)\n",
    "second_batch = next(data_iter)\n",
    "print(second_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Creating token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.9269,  1.4873, -0.4974],\n",
      "        [ 0.4396, -0.7581,  1.0783],\n",
      "        [ 0.8008,  1.6806,  0.3559],\n",
      "        [-0.6866,  0.6105,  1.3347],\n",
      "        [-0.2316,  0.0418, -0.2516],\n",
      "        [ 0.8599, -0.3097, -0.3957]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.tensor([2, 3, 5, 1])\n",
    "\n",
    "vocab_size = 6\n",
    "output_dim = 3  # create embeddings of size 3\n",
    "\n",
    "torch.manual_seed(42)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights above have been randomly initialized.\n",
    "The values will get optimized during LLM training, as part of the LLM optimization.\n",
    "6 rows with 3 columns. One row for each of the six possible tokens in the vocabulary, and one column for each of the three embedding dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6866,  0.6105,  1.3347]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.tensor([3])))  # applying embedding layer to token id 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the output is identical to the index 3 in the weights.\n",
    "This is because the embedding layer is basically like a lookup from the embedding layer's weights via the token ID.\n",
    "\n",
    "The embedding layer here is like a more efficient way to implement one-hot encoding, followed by matrix multiplication in a fully connected layer.\n",
    "And that's also why we can view it as a neural network layer that can be optimized via backprop.\n",
    "\n",
    "Sebastian provided a great notebook that explains this relationship [here](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/03_bonus_embedding-vs-matmul/embeddings-and-linear-layers.ipynb).\n",
    "In it, he explains that embedding layers in PyTorch do the same as linear layers that perform matrix multiplications. We use embedding layers for computational efficiency.\n",
    "\n",
    "Above we've discussed how the embedding is basically like a lookup, and that this is comparable to one-hot and a matmul for a linear layer. So say we have the `nn.Linear` layer on a one-hot encoded representation.\n",
    "So the categories are the various token ids we have available, and we've one-hot encoded those to be binary attributes. Therefore, we have as many one-hot features as tokens in our vocabulary.\n",
    "Given a token ID, we'd encode it such as a vector with a binary 1 (hot) in its attribute and 0 elsewhere.\n",
    "Performing a matrix multiplication of that vector with our linear layer's weights gives us the embeddings for that exact token, equivalent to the lookup.\n",
    "\n",
    "Mathematically, we can represent this as:\n",
    "\n",
    "$\\mathbf{e} = \\mathbf{x}^T \\mathbf{W}$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{e}$ is the resulting embedding vector\n",
    "- $\\mathbf{x}$ is the one-hot encoded input vector\n",
    "- $\\mathbf{W}$ is the weight matrix of the linear layer (or embedding matrix)\n",
    "\n",
    "For example, if we have a vocabulary size of 6 and an embedding dimension of 3:\n",
    "\n",
    "$\\begin{bmatrix}0 & 0 & 1 & 0 & 0 & 0\\end{bmatrix} \\begin{bmatrix}w_{11} & w_{12} & w_{13} \\\\ w_{21} & w_{22} & w_{23} \\\\ w_{31} & w_{32} & w_{33} \\\\ w_{41} & w_{42} & w_{43} \\\\ w_{51} & w_{52} & w_{53} \\\\ w_{61} & w_{62} & w_{63}\\end{bmatrix} = \\begin{bmatrix}w_{31} & w_{32} & w_{33}\\end{bmatrix}$\n",
    "\n",
    "This operation effectively selects the third row of the weight matrix, which is equivalent to looking up the embedding for the third token in our vocabulary.\n",
    "\n",
    "The embedding layer can also be thought of as a hashtable lookup. In this case, we can represent it as:\n",
    "\n",
    "```python\n",
    "embedding = hashtable[token_id]\n",
    "```\n",
    "\n",
    "Where:\n",
    "- embedding is the resulting embedding vector\n",
    "- hashtable is a dictionary-like structure containing the embeddings\n",
    "- token_id is the ID of the token we want to look up\n",
    "\n",
    "For our example with a vocabulary size of 6 and an embedding dimension of 3, we could represent this as:\n",
    "\n",
    "```python\n",
    "hashtable = {\n",
    "    0: [w11, w12, w13],\n",
    "    1: [w21, w22, w23],\n",
    "    2: [w31, w32, w33],\n",
    "    3: [w41, w42, w43],\n",
    "    4: [w51, w52, w53],\n",
    "    5: [w61, w62, w63]\n",
    "}\n",
    "```\n",
    "\n",
    "Then, to get the embedding for token ID 2, we would simply do:\n",
    "\n",
    "```python\n",
    "embedding = hashtable[2]  # This would return [w31, w32, w33]\n",
    "```\n",
    "\n",
    "This hashtable lookup approach is conceptually similar to the embedding layer and provides\n",
    "another way to understand how embeddings work. However, the actual implementation in\n",
    "PyTorch uses more optimized methods for efficiency and to enable gradient flow for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8008,  1.6806,  0.3559],\n",
      "        [-0.6866,  0.6105,  1.3347],\n",
      "        [ 0.8599, -0.3097, -0.3957],\n",
      "        [ 0.4396, -0.7581,  1.0783]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 Encoding word positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=max_length,\n",
    "    stride=max_length, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape) # 8 text samples, 4 tokens each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We embedded each of the tokens into a 256 dimensional vector.\n",
    "8 samples in our batch (4 text samples), 4 tokens per sample, and 256 embedding dimensions for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "# A GPT model's absolute embedding approach:\n",
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input is usually a placeholder vector containing a sequence of numbers 0, 1, ..., n, where n is the maximum input length.\n",
    "\n",
    "`context_length` represents the supported input size for the LLM.\n",
    "We set it to `max_length` here.\n",
    "In practice, the input text can be longer than the supported context length--then we'd have to truncate the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
