{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autodiff\n",
    "\n",
    "These are my notes on an appendix of Hands on Machine Learning 3rd Edition.\n",
    "\n",
    "---\n",
    "\n",
    "# Automatic Differentiation\n",
    "We have function:\n",
    "\n",
    "$$f(x, y) = x^2 + y+2$$\n",
    "\n",
    "We need the partial derivatives $\\frac{df}{dx}$ and $\\frac{df}{dy}$.\n",
    "Usually done to do gradient descent (or another optimization algo).\n",
    "\n",
    "Can either:\n",
    "- Use sympy to calculate the derivatives // manual differentiation\n",
    "- Finite difference approximation\n",
    "- Use autograd to calculate the derivatives // automatic differentiation\n",
    "\n",
    "But there are a few ways to do autodiff:\n",
    "- Forward mode (calculate derivative of each variable)\n",
    "- Reverse mode (calculate derivative of each function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Differentiation\n",
    "\n",
    "Pick up a piece of paper and use calculus to derive the appropriate equation.\n",
    "This gets incredibly tedious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite Difference Approximation\n",
    "\n",
    "Unfortunately, this is imprecise and slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.000039999805264 10.000000000331966\n"
     ]
    }
   ],
   "source": [
    "def f(x, y):\n",
    "    return x**2 * y + y + 2\n",
    "\n",
    "\n",
    "def derivative(f, x, y, x_eps, y_eps):\n",
    "    return (f(x + x_eps, y + y_eps) - f(x, y)) / (x_eps + y_eps)\n",
    "\n",
    "\n",
    "df_dx = derivative(f, 3, 4, 0.00001, 0)\n",
    "df_dy = derivative(f, 3, 4, 0, 0.00001)\n",
    "\n",
    "print(df_dx, df_dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward-Mode Autodiff\n",
    "\n",
    "Algo goes through computation graph from inputs to outputs (hence 'forward').\n",
    "Starts by getting partial derivatives of leaf nodes.\n",
    "Then uses chain rule to calculate derivatives of other nodes.\n",
    "\n",
    "Forward-Mode takes one computation graph and produces another.\n",
    "This is called symbolic differentiation.\n",
    "A nice byproduct of this is that we can reuse the output computation graph to calculate the derivatives of the given function for any value of $x$ and $y$.\n",
    "And we can run it again on the output graph to get second-order derivatives (and so on).\n",
    "\n",
    "But we can also do forward-mode autodiff without creating a graph (so numerically, not symbolically) by computing intermediate results on the fly. Can use dual numbers for this.\n",
    "\n",
    "The major flaw of forward-mode autodiff is that it's not efficient for functions with many inputs. Not great for deep learning, where there are so many parameters.\n",
    "This is where reverse-mode autodiff comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse-Mode Autodiff\n",
    "\n",
    "Goes through graph in forward direction to compute values of each node, and then does a second, reverse pass to compute all the partial derivatives.\n",
    "\n",
    "We gradually go through the graph in reverse, computing the partial derivatives of the function, w.r.t each consecutive node, until we reach the inputs.\n",
    "This uses the chain rule, and is called reverse accumulation.\n",
    "\n",
    "Reverse-mode autodiff is efficient for functions with many inputs, but not so much for many outputs.\n",
    "It requires only one forward pass & one reverse pass per output to compute all the partial derivatives for all outputs, w.r.t the inputs.\n",
    "When we train neural nets, there's only one output (the loss), but many inputs.\n",
    "\n",
    "It can also handle functions that aren't entirely differentiable - but only if you only ask it to compute the partial derivatives at points where the function is differentiable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
