{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Attending to different parts of the input with self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 A simple self attention mechanism without trainable weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4300, 0.1500, 0.8900],\n",
       "        [0.5500, 0.8700, 0.6600],\n",
       "        [0.5700, 0.8500, 0.6400],\n",
       "        [0.2200, 0.5800, 0.3300],\n",
       "        [0.7700, 0.2500, 0.1000],\n",
       "        [0.0500, 0.8000, 0.5500]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Inputs are the embeddings of the words in the sentence\n",
    "inputs = torch.tensor(\n",
    "    [\n",
    "        [0.43, 0.15, 0.89],  # Your     (x^1)\n",
    "        [0.55, 0.87, 0.66],  # journey  (x^2)\n",
    "        [0.57, 0.85, 0.64],  # starts   (x^3)\n",
    "        [0.22, 0.58, 0.33],  # with     (x^4)\n",
    "        [0.77, 0.25, 0.10],  # one      (x^5)\n",
    "        [0.05, 0.80, 0.55],\n",
    "    ]  # step     (x^6)\n",
    ")\n",
    "\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the second token, `journey`, as the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = inputs[1]\n",
    "attention_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attention_scores_2[i] = torch.dot(query, x_i)\n",
    "\n",
    "attention_scores_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we normalize each of the attention scores.\n",
    "\n",
    "We want to ensure that the sum of the attention weights is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "# Notice how we computed use the scores to compute the weights\n",
    "attention_weights_2_tmp = attention_scores_2 / attention_scores_2.sum()\n",
    "print(\"Attention weights:\", attention_weights_2_tmp)\n",
    "print(\"Sum:\", attention_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it's better to use the `softmax` function for normalization. It's better at handling extreme values & gives better gradient properties during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attention_weights_2 = torch.softmax(attention_scores_2, dim=0)\n",
    "print(\"Attention weights:\", attention_weights_2)\n",
    "print(\"Sum:\", attention_weights_2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to calculate the context vector by multiplying the embedded input tokens with the corresponding attention weights & summing the resulting vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 | 0.14 * tensor([0.4300, 0.1500, 0.8900])\n",
      "1 | 0.24 * tensor([0.5500, 0.8700, 0.6600])\n",
      "2 | 0.23 * tensor([0.5700, 0.8500, 0.6400])\n",
      "3 | 0.12 * tensor([0.2200, 0.5800, 0.3300])\n",
      "4 | 0.11 * tensor([0.7700, 0.2500, 0.1000])\n",
      "5 | 0.16 * tensor([0.0500, 0.8000, 0.5500])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.4419, 0.6515, 0.5683])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = inputs[1]\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    print(f\"{i} | {attention_weights_2[i]:.2f} * {x_i}\")\n",
    "    context_vec_2 += attention_weights_2[i] * x_i\n",
    "\n",
    "context_vec_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just me doing retrieval practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4421, 0.5931, 0.5790])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute attention scores\n",
    "query = inputs[0]\n",
    "attn_scores_1 = torch.zeros(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_1[i] = torch.dot(query, x_i)\n",
    "\n",
    "# Normalize - compute attenttion weights\n",
    "attn_weights_1 = torch.softmax(attn_scores_1, dim=0)\n",
    "\n",
    "# Compute context vector\n",
    "context_vec_1 = torch.zeros(query.shape)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec_1 += attn_weights_1[i] * x_i\n",
    "\n",
    "context_vec_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Computing attention weights for all input tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
       "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
       "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
       "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
       "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
       "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores = torch.empty((inputs.shape[0], inputs.shape[0]))\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "attn_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element in the `attn_scores` tensor represents an attention score between each pair of inputs.\n",
    "\n",
    "You can imagine it being a matrix like this, excluding the labels:\n",
    "| | Your | journey | starts | with | one | step |\n",
    "|---|---|---|---|---|---|---|\n",
    "| Your | 0.9995 | 0.9544 | 0.9422 | 0.4753 | 0.4576 | 0.6310 |\n",
    "| journey | 0.9544 | 1.4950 | 1.4754 | 0.8434 | 0.7070 | 1.0865 |\n",
    "| starts | 0.9422 | 1.4754 | 1.4570 | 0.8296 | 0.7154 | 1.0605 |\n",
    "| with | 0.4753 | 0.8434 | 0.8296 | 0.4937 | 0.3474 | 0.6565 |\n",
    "| one | 0.4576 | 0.7070 | 0.7154 | 0.3474 | 0.6654 | 0.2935 |\n",
    "| step | 0.6310 | 1.0865 | 1.0605 | 0.6565 | 0.2935 | 0.9450 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
       "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
       "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
       "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
       "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
       "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A faster way:\n",
    "attn_scores = inputs @ inputs.T  # or torch.matmul\n",
    "attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
       "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
       "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
       "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
       "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
       "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dim=-1 means the last dimension.\n",
    "# For this rank 2 tensor, it means we're applying softmax along the second dimension of [rows, columns]. That is,\n",
    "# we're normalizing across the columns, so the values in each row (summing over the column dimension) sum up to 1.\n",
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4421, 0.5931, 0.5790],\n",
       "        [0.4419, 0.6515, 0.5683],\n",
       "        [0.4431, 0.6496, 0.5671],\n",
       "        [0.4304, 0.6298, 0.5510],\n",
       "        [0.4671, 0.5910, 0.5266],\n",
       "        [0.4177, 0.6503, 0.5645]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vecs = attn_weights @ inputs\n",
    "context_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Implementing self-attention with trainable weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 Computing the attention weights step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]  # input embedding size\n",
    "d_out = 2  # output embedding size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`requires_grad=False` to reduce clutter. If we were using the weight matrices for model training, we'd set it to `True` during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0760, 1.7344])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "\n",
    "query_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 2]), torch.Size([6, 2]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "\n",
    "keys.shape, values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've projected the six input tokens from a three-dimensional onto a two-dimensional embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3338)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys_2 = keys[1]\n",
    "attn_scores_22 = query_2.dot(keys_2)\n",
    "attn_scores_22  # unnormalized attention score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.7084, 3.3338, 3.3013, 1.7563, 1.7869, 2.1966])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generalized:\n",
    "attn_scores_2 = query_2 @ keys.T\n",
    "attn_scores_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From attention scores to attention weights.\n",
    "\n",
    "Scale the attention scores by dividing them by the sqrt of the embedding dimension of the keys & then using the softmax fn.\n",
    "\n",
    "We scale by the embedding dimension to improve training performance by avoiding small gradients.\n",
    "\n",
    "Large dot products can lead to very small gradients during backprop due to softmax. As dot products increase, softmax becomes more like a step function, leading to gradients near zero. These can slow down training / cause it to stagnate.\n",
    "\n",
    "We call this self-attention mechanism \"scaled-dot product attention\" due to this scaling by the sqrt of the embedding dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1723, 0.2681, 0.2620, 0.0879, 0.0898, 0.1200])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "attn_weights_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.4201, 0.8892])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "\n",
    "context_vec_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 Implementing a compact self-attention Python class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in: int, d_out: int):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3751, 0.8610],\n",
       "        [1.4201, 0.8892],\n",
       "        [1.4198, 0.8890],\n",
       "        [1.3533, 0.8476],\n",
       "        [1.3746, 0.8606],\n",
       "        [1.3620, 0.8532]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "sa_v1(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `nn.Linear` layers instead, which effectively perform matmuls when bias units are disabled.\n",
    "And linear layers have an optimized weight initialization scheme, meaning more stable and effective training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in: int, d_out: int, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3755, 0.2777],\n",
       "        [0.3761, 0.2831],\n",
       "        [0.3761, 0.2833],\n",
       "        [0.3768, 0.2763],\n",
       "        [0.3754, 0.2836],\n",
       "        [0.3772, 0.2746]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "\n",
    "# Exercise 3.1: checking that nn.Linear (bias=False) is similar to nn.Parameter, except for weight initialization\n",
    "# sa_v2.W_query.weight = nn.Parameter(sa_v1.W_query.T)\n",
    "# sa_v2.W_key.weight = nn.Parameter(sa_v1.W_key.T)\n",
    "# sa_v2.W_value.weight = nn.Parameter(sa_v1.W_value.T)\n",
    "\n",
    "sa_v2(inputs)  # different outputs due to different weight initialization schemes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Hiding future words with causal attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some tasks, we want self-attention to only consider tokens appearing prior to the current position when predicting tokens in a sequence.\n",
    "\n",
    "Causal attention is also known as masked attention.\n",
    "It's a special form of self-attention that restricts the model to only consider previous and current inputs in a sequence.\n",
    "We essentially mask out future tokens - tokens that come after the current token in the input.\n",
    "Mask attention weights above the diagonal, and normalize the non-masked attention weights, so they sum to 1 in each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1605, 0.1726, 0.1714, 0.1681, 0.1473, 0.1801],\n",
       "        [0.1627, 0.1780, 0.1758, 0.1648, 0.1306, 0.1880],\n",
       "        [0.1625, 0.1782, 0.1759, 0.1648, 0.1302, 0.1885],\n",
       "        [0.1661, 0.1726, 0.1715, 0.1654, 0.1475, 0.1768],\n",
       "        [0.1596, 0.1777, 0.1755, 0.1664, 0.1312, 0.1896],\n",
       "        [0.1682, 0.1715, 0.1707, 0.1648, 0.1511, 0.1738]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "mask_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1605, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1627, 0.1780, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1625, 0.1782, 0.1759, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1661, 0.1726, 0.1715, 0.1654, 0.0000, 0.0000],\n",
       "        [0.1596, 0.1777, 0.1755, 0.1664, 0.1312, 0.0000],\n",
       "        [0.1682, 0.1715, 0.1707, 0.1648, 0.1511, 0.1738]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using * on two matrices of the same shape with PyTorch does elementwise multiplication.\n",
    "masked_simple = attn_weights * mask_simple\n",
    "masked_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4775, 0.5225, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3146, 0.3450, 0.3405, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2459, 0.2555, 0.2538, 0.2448, 0.0000, 0.0000],\n",
       "        [0.1969, 0.2193, 0.2165, 0.2053, 0.1619, 0.0000],\n",
       "        [0.1682, 0.1715, 0.1707, 0.1648, 0.1511, 0.1738]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_sums = masked_simple.sum(dim=-1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "masked_simple_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, we'll use a trick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0508,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "        [ 0.2157,  0.3428,    -inf,    -inf,    -inf,    -inf],\n",
       "        [ 0.2163,  0.3467,  0.3282,    -inf,    -inf,    -inf],\n",
       "        [ 0.1257,  0.1799,  0.1707,  0.1191,    -inf,    -inf],\n",
       "        [ 0.1667,  0.3193,  0.3012,  0.2258, -0.1098,    -inf],\n",
       "        [ 0.1269,  0.1548,  0.1475,  0.0978, -0.0247,  0.1731]],\n",
       "       grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4775, 0.5225, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3146, 0.3450, 0.3405, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2459, 0.2555, 0.2538, 0.2448, 0.0000, 0.0000],\n",
       "        [0.1969, 0.2193, 0.2165, 0.2053, 0.1619, 0.0000],\n",
       "        [0.1682, 0.1715, 0.1707, 0.1648, 0.1511, 0.1738]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / keys.shape[-1] ** 0.5, dim=-1)\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.triu(torch.ones(context_length, context_length), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 2., 2., 2., 2.],\n",
       "        [2., 0., 2., 0., 2., 0.],\n",
       "        [0., 0., 2., 2., 2., 0.],\n",
       "        [2., 2., 0., 2., 0., 2.],\n",
       "        [2., 0., 2., 2., 2., 2.],\n",
       "        [2., 2., 2., 0., 2., 0.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "dropout = torch.nn.Dropout(0.5)  # usually 0.1 or 0.2 for training GPT models\n",
    "example = torch.ones(6, 6)\n",
    "dropout(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~50% were scaled to zero. To compensate for the reduction in active elements, the rest were scaled up by a factor of $1/0.5=2$.\n",
    "This is to maintain the overall balance of the weights, so the average influence of attention mechanisms is consistent both during training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.9551, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.6292, 0.0000, 0.6809, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.5110, 0.0000, 0.4895, 0.0000, 0.0000],\n",
       "        [0.3938, 0.4387, 0.4331, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3364, 0.0000, 0.0000, 0.0000, 0.0000, 0.3476]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to ensure the code can handle batches of more than one input.\n",
    "But we just use the input twice here for convencience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 3])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self, d_in: int, d_out: int, context_length: int, dropout: float, qkv_bias=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Useful because the buffer is auto-moved to the appropriate device (CPU/GPU) with our model (e.g. when training)\n",
    "        self.register_buffer(\n",
    "            \"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [batch, num_tokens, d_in]\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(\n",
    "            1, 2\n",
    "        )  # Keep batch dim at position 0, but transpose dim 1 and 2\n",
    "        # Trailing _ means inplace. Using it to avoid unnecessary memory copies.\n",
    "        attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attn_weights = torch.softmax(masked / keys.shape[-1] ** 0.5, dim=-1)\n",
    "        attn_weights_dropped = self.dropout(attn_weights)\n",
    "        context_vec = attn_weights_dropped @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 2])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "context_vecs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ Resulting context vector is now a three-dimensional tensor where each token is represented by a two-dimensional embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.6 Extending single-head attention to multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_in: int,\n",
    "        d_out: int,\n",
    "        context_length: int,\n",
    "        dropout: float,\n",
    "        num_heads: int,\n",
    "        qkv_bias=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [\n",
    "                CausalAttention(d_in, d_out, context_length, dropout, qkv_bias)\n",
    "                for _ in range(num_heads)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4429, 0.1077, 0.5473, 0.3307],\n",
      "         [0.4656, 0.2597, 0.3420, 0.2234],\n",
      "         [0.4732, 0.3030, 0.2818, 0.1894],\n",
      "         [0.4135, 0.2921, 0.2105, 0.1521],\n",
      "         [0.4078, 0.2567, 0.2252, 0.1357],\n",
      "         [0.3772, 0.2746, 0.1709, 0.1215]],\n",
      "\n",
      "        [[0.4429, 0.1077, 0.5473, 0.3307],\n",
      "         [0.4656, 0.2597, 0.3420, 0.2234],\n",
      "         [0.4732, 0.3030, 0.2818, 0.1894],\n",
      "         [0.4135, 0.2921, 0.2105, 0.1521],\n",
      "         [0.4078, 0.2567, 0.2252, 0.1357],\n",
      "         [0.3772, 0.2746, 0.1709, 0.1215]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape=torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "context_length = batch.shape[1]\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(f\"{context_vecs.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first dimension is `2` because we have two samples in our batch.\n",
    "- The second dimension denotes the 6 tokens in each input.\n",
    "- The third dimension is the 4-dimensional embeddings of each token.\n",
    "\n",
    "`[batch_size, context_length, embedding_dimensions]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.2 Returning two-dimensional embedding vectors**\n",
    "> Change the input arguments for the `MultiHeadAttentionWrapper(..., num_ heads=2)` call such that the output context vectors are two-dimensional instead of four dimensional while keeping the setting `num_heads=2`. Hint: You donâ€™t have to modify the class implementation; you just have to change one of the other input arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.4788,  0.2593],\n",
      "         [ 0.6509,  0.0176],\n",
      "         [ 0.6989, -0.0575],\n",
      "         [ 0.6378, -0.0745],\n",
      "         [ 0.5993, -0.1042],\n",
      "         [ 0.5900, -0.1018]],\n",
      "\n",
      "        [[ 0.4788,  0.2593],\n",
      "         [ 0.6509,  0.0176],\n",
      "         [ 0.6989, -0.0575],\n",
      "         [ 0.6378, -0.0745],\n",
      "         [ 0.5993, -0.1042],\n",
      "         [ 0.5900, -0.1018]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape=torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "d_in, d_out = 3, 1\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(f\"{context_vecs.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing `d_out` to 1 means the context vector will be of `2*1=2` dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.2 Implementing multi-head attention with weight splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_in: int,\n",
    "        d_out: int,\n",
    "        context_length: int,\n",
    "        dropout: float,\n",
    "        num_heads: int,\n",
    "        qkv_bias=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = (\n",
    "            d_out // num_heads\n",
    "        )  # Reduces projection dim to match desired output dim\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # To combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        # Tensor shape (b, num_tokens, d_out)\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        keys = keys.view(\n",
    "            b, num_tokens, self.num_heads, self.head_dim\n",
    "        )  # implicitly split the matrix by adding num_heads dimension, then unroll the last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transposes from shape (b, num_tokens, num_heads, head_dim) to (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(\n",
    "            2, 3\n",
    "        )  # compute dot product for each head\n",
    "        mask_bool = self.mask.bool()[\n",
    "            :num_tokens, :num_tokens\n",
    "        ]  # masks truncated to the number of tokens\n",
    "\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)  # uses mask to fill attn scores\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = (attn_weights @ values).transpose(\n",
    "            1, 2\n",
    "        )  # tensor shape: (b, num_tokens, n_heads, head_dim)\n",
    "\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # optional linear projection\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.6380,  0.3370],\n",
      "         [-0.7576,  0.2926],\n",
      "         [-0.7891,  0.2779],\n",
      "         [-0.7887,  0.2770],\n",
      "         [-0.6782,  0.2563],\n",
      "         [-0.7425,  0.2639]],\n",
      "\n",
      "        [[-0.6380,  0.3370],\n",
      "         [-0.7576,  0.2926],\n",
      "         [-0.7891,  0.2779],\n",
      "         [-0.7887,  0.2770],\n",
      "         [-0.6782,  0.2563],\n",
      "         [-0.7425,  0.2639]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape=torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "d_in, d_out = 3, 2\n",
    "context_length = batch.shape[1]\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2, qkv_bias=False)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(f\"{context_vecs.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE 3.3 INITIALIZING GPT-2 SIZE ATTENTION MODULES**\n",
    "> Using the MultiHeadAttention class, initialize a multi-head attention module that has the same number of attention heads as the smallest GPT-2 model (12 attention heads). Also ensure that you use the respective input and output embedding sizes similar to GPT-2 (768 dimensions). Note that the smallest GPT-2 model supports a context length of 1,024 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiHeadAttention(\n",
       "  (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "  (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "  (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_heads = 12\n",
    "d_in, d_out = 768, 768\n",
    "context_length = 1024\n",
    "\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, n_heads, False)\n",
    "mha"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
