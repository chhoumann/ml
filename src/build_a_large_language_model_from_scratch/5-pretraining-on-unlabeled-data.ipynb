{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Evaluating generative models\n",
    "### 5.1.1 Using GPT to generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from build_a_large_language_model_from_scratch.lib.GPTModel import GPTModel\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256, # intentionally shortening it to reduce computational demands of training\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12, \n",
    "    \"drop_rate\": 0.1, # it's possible and common to set dropout to 0\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "cfg = GPT_CONFIG_124M\n",
    "\n",
    "model = GPTModel(\n",
    "    context_length=cfg[\"context_length\"],\n",
    "    drop_rate=cfg[\"drop_rate\"],\n",
    "    emb_dim=cfg[\"emb_dim\"],\n",
    "    n_heads=cfg[\"n_heads\"],\n",
    "    n_layers=cfg[\"n_layers\"],\n",
    "    vocab_size=cfg[\"vocab_size\"],\n",
    "    qkv_bias=cfg[\"qkv_bias\"]\n",
    ")\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      "Every effort moves you PharmacET structureguide attribution Ped prophets Theme Downing surrounded\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "from build_a_large_language_model_from_scratch.lib.generate import generate_text_simple\n",
    "\n",
    "def text_to_token_ids(text: str, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # `unsqueeze(0)` adds batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=cfg[\"context_length\"]\n",
    ")\n",
    "print(f\"Output text:\\n{token_ids_to_text(token_ids, tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 Calculating the text generation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probas.shape=torch.Size([2, 3, 50257])\n",
      "token_ids=tensor([[[42181],\n",
      "         [44128],\n",
      "         [42004]],\n",
      "\n",
      "        [[34515],\n",
      "         [  525],\n",
      "         [27659]]])\n",
      "token_ids.shape=torch.Size([2, 3, 1])\n",
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  neurot avalancheifestyle\n",
      "Text 1: tensor([1.1494e-05, 2.3580e-05, 1.9614e-05])\n",
      "Text 2: tensor([5.8698e-05, 1.3303e-05, 1.2768e-05])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor([\n",
    "    [16833, 3626, 6100], # \"every effort moves\"\n",
    "      [40, 1107, 58]])   # \"I really like\"\n",
    "\n",
    "targets = torch.tensor([\n",
    "    [3626, 6100, 345], # \" effort moves you\"\n",
    "    [1107, 588, 11311] # \" really like chocolate\"\n",
    "])\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1)\n",
    "print(f\"{probas.shape=}\")\n",
    "\n",
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(f\"{token_ids=}\")\n",
    "print(f\"{token_ids.shape=}\")\n",
    "\n",
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")\n",
    "\n",
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(f\"Text 1: {target_probas_1}\")\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(f\"Text 2: {target_probas_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' neurot avalancheifestyle', ' laureperclair']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer.decode(o) for o in token_ids.flatten(1).tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-11.3737, -10.6551, -10.8393,  -9.7431, -11.2275, -11.2686])\n"
     ]
    }
   ],
   "source": [
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.8512)\n"
     ]
    }
   ],
   "source": [
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.8512)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits.shape=torch.Size([2, 3, 50257])\n",
      "targets.shape=torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(f\"{logits.shape=}\")\n",
    "print(f\"{targets.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 50257])\n",
      "torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "# Need to flatten for `cross_entropy` to work, so we combien them over the batch dimension:\n",
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "print(f\"{logits_flat.shape}\")\n",
    "print(f\"{targets_flat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.8512)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(51596.8203)\n"
     ]
    }
   ],
   "source": [
    "# Perplexity\n",
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3 Calculating the training and validation set losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./data/the-verdict.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_characters=20479\n",
      "total_tokens=5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(f\"{total_characters=}\")\n",
    "print(f\"{total_tokens=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from build_a_large_language_model_from_scratch.lib.dataloader import create_dataloader_v1\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# We use a small batch size to reduce computational resource demand because we're working with a small dataset.\n",
    "# Using batch sizes of 1024 or larger is not uncommon.\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=cfg[\"context_length\"],\n",
    "    stride=cfg[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=cfg[\"context_length\"],\n",
    "    stride=cfg[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "# Check data loaders:\n",
    "print(\"Train loader\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "\n",
    "print(\"\\nValidation loader\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.flatten(0, 1), target_batch.flatten()\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    \n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 11.025788942972818\n",
      "Validation loss: 11.056291580200195\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Training an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_model_simple(\n",
    "    model,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    num_epochs: int,\n",
    "    eval_freq: int,\n",
    "    eval_iter: int,\n",
    "    start_context: str,\n",
    "    tokenizer,\n",
    "):\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()  # reset loss gradients from previous iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()  # calculate loss gradients\n",
    "            optimizer.step()  # update model weights using the loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(\n",
    "                    f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                    f\"Train loss {train_loss:.3f}, \"\n",
    "                    f\"Val loss {val_loss:.3f}\"\n",
    "                )\n",
    "\n",
    "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "def evaluate_model(model, train_loader: DataLoader, val_loader: DataLoader, device, eval_iter: int):\n",
    "    model.eval()  # to disable dropout during evaluation\n",
    "    with torch.no_grad():  # to disable gradient tracking, it's not required (reduce computational overhead)\n",
    "        train_loss = calc_loss_loader(\n",
    "            train_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "        val_loss = calc_loss_loader(\n",
    "            val_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.816, Val loss 9.924\n",
      "Ep 1 (Step 000005): Train loss 8.064, Val loss 8.337\n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Ep 2 (Step 000010): Train loss 6.620, Val loss 7.051\n",
      "Ep 2 (Step 000015): Train loss 6.042, Val loss 6.599\n",
      "Every effort moves you, and,, and,,,,,,, and,.                                   \n",
      "Ep 3 (Step 000020): Train loss 5.510, Val loss 6.531\n",
      "Ep 3 (Step 000025): Train loss 5.335, Val loss 6.374\n",
      "Every effort moves you, and to the to the of the picture to the picture.                                     \n",
      "Ep 4 (Step 000030): Train loss 4.685, Val loss 6.240\n",
      "Ep 4 (Step 000035): Train loss 4.718, Val loss 6.350\n",
      "Every effort moves you of the picture.  \"I had to the picture.         \"I \"I to the the picture--as I had the donkey of the donkey of the donkey of the of the picture of the\n",
      "Ep 5 (Step 000040): Train loss 3.860, Val loss 6.133\n",
      "Every effort moves you know it was not to see a little the fact the fact of the last I had been--and it--and here are the fact of the fact of his pictures--I had been.  \"I had been--I had been the fact\n",
      "Ep 6 (Step 000045): Train loss 3.521, Val loss 6.184\n",
      "Ep 6 (Step 000050): Train loss 2.989, Val loss 6.129\n",
      "Every effort moves you know it was not that the picture.                                          \n",
      "Ep 7 (Step 000055): Train loss 2.859, Val loss 6.137\n",
      "Ep 7 (Step 000060): Train loss 2.115, Val loss 6.132\n",
      "Every effort moves you know,\" was not that the picture for a smile that I felt to me--so it was no great, the fact, the fact that, I was his pictures.                \n",
      "Ep 8 (Step 000065): Train loss 1.699, Val loss 6.188\n",
      "Ep 8 (Step 000070): Train loss 1.404, Val loss 6.211\n",
      "Every effort moves you?\"  \"Yes--I glanced after him, and uncertain.  \"I looked--I looked up, I felt to see a smile behind his close grayish beard--as if he had the donkey. \"There were days when I\n",
      "Ep 9 (Step 000075): Train loss 1.072, Val loss 6.265\n",
      "Ep 9 (Step 000080): Train loss 0.812, Val loss 6.284\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"         He placed them at my elbow and continued to wander up and down the room, and now\n",
      "Ep 10 (Step 000085): Train loss 0.587, Val loss 6.413\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "model = GPTModel(\n",
    "    vocab_size=cfg[\"vocab_size\"],\n",
    "    context_length=cfg[\"context_length\"],\n",
    "    drop_rate=cfg[\"drop_rate\"],\n",
    "    emb_dim=cfg[\"emb_dim\"],\n",
    "    n_heads=cfg[\"n_heads\"],\n",
    "    n_layers=cfg[\"n_layers\"],\n",
    "    qkv_bias=cfg[\"qkv_bias\"]\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.0004, weight_decay=0.1\n",
    ")\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAEiCAYAAAA21pHjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABXR0lEQVR4nO3deVxU1fvA8c8M+7DLjsiiooDiihriUmkumaVZtvA1bLNyzyzrW5naYqWZaaVZv7S+ZbZqliuaueCGC4qJO4gLiIqyyjrn98fo4KipKDgDPu/X676Yuffce585wDz3nLscjVJKIYQQQgiLpDV3AEIIIYT4d5KohRBCCAsmiVoIIYSwYJKohRBCCAsmiVoIIYSwYJKohRBCCAsmiVoIIYSwYJKohRBCCAsmiVoIIYSwYJKohagF0tLS0Gg0JCUlmTsUIUQVk0QthIXQaDRXncaNG2fuEIUQZmBt7gCEEAYZGRnG1z/++CNjx45l7969xnlOTk7mCEsIYWbSohbCQvj6+honV1dXNBqN8b23tzdTpkwhICAAOzs7WrRowdKlS/91W+Xl5Tz11FOEhYWRnp4OwO+//06rVq2wt7enfv36jB8/nrKyMuM6Go2Gr776ir59+6LT6QgNDWXhwoXG5WfOnCE2NhYvLy8cHBwIDQ1l9uzZ/xrDL7/8QmRkJA4ODnh4eNC1a1cKCgqMy7/66ivCw8Oxt7cnLCyMzz//3GT9I0eO0L9/f9zc3KhTpw4PPPAAaWlpxuUDBw6kT58+TJ48GT8/Pzw8PBgyZAilpaXXXedC1AhKCGFxZs+erVxdXY3vp0yZolxcXNQPP/yg9uzZo1555RVlY2Oj9u3bp5RSKjU1VQFq+/btqqioSPXt21e1bNlSZWVlKaWUWrNmjXJxcVFz5sxRBw8eVMuXL1fBwcFq3Lhxxn0AKiAgQM2dO1ft379fDR8+XDk5OanTp08rpZQaMmSIatGihUpMTFSpqakqPj5eLVy48IrxHz9+XFlbW6spU6ao1NRUtXPnTvXZZ5+pvLw8pZRS3333nfLz81O//vqrOnTokPr1119VnTp11Jw5c5RSSpWUlKjw8HD11FNPqZ07d6rdu3erxx9/XDVu3FgVFxcrpZSKi4tTLi4u6vnnn1cpKSnqjz/+UDqdTs2aNatqfxlCmJkkaiEs0KWJ2t/fX7377rsmZdq0aaMGDx6slKpI1GvXrlVdunRRHTp0UGfPnjWW7dKli3rvvfdM1v/f//6n/Pz8jO8B9cYbbxjf5+fnK0AtWbJEKaVU79691ZNPPnld8W/dulUBKi0t7YrLGzRooObOnWsy7+2331bR0dHG2Bo3bqz0er1xeXFxsXJwcFDLli1TShkSdVBQkCorKzOWefjhh9UjjzxyXTEKUVPIOWohLFxubi7Hjx8nJibGZH5MTAw7duwwmffYY48REBDAX3/9hYODg3H+jh07SEhI4N133zXOKy8vp6ioiMLCQnQ6HQDNmjUzLnd0dMTFxYWsrCwAXnjhBfr168e2bdvo1q0bffr0oX379leMuXnz5nTp0oXIyEi6d+9Ot27deOihh3B3d6egoICDBw/y9NNP8+yzzxrXKSsrw9XV1RjvgQMHcHZ2NtluUVERBw8eNL5v0qQJVlZWxvd+fn4kJydfpTaFqHkkUQtRi9x777189913bNiwgbvvvts4Pz8/n/Hjx/Pggw9eto69vb3xtY2NjckyjUaDXq8HoGfPnhw+fJjFixcTHx9Ply5dGDJkCJMnT75sm1ZWVsTHx7N+/XqWL1/O9OnTef3119m0aZPxoODLL7+kXbt2l613Id7WrVvz/fffX7ZtLy+v64pXiNpCErUQFs7FxQV/f38SEhLo3LmzcX5CQgJt27Y1KfvCCy/QtGlT7r//fhYtWmQs36pVK/bu3UvDhg1vKhYvLy/i4uKIi4ujY8eOvPzyy1dM1GBImjExMcTExDB27FiCgoKYP38+o0aNwt/fn0OHDhEbG3vFdVu1asWPP/6It7c3Li4uNxWzEDWdJGohaoCXX36Zt956iwYNGtCiRQtmz55NUlLSFVucw4YNo7y8nPvuu48lS5bQoUMHxo4dy3333UdgYCAPPfQQWq2WHTt2sGvXLt55553rimHs2LG0bt2aJk2aUFxczJ9//kl4ePgVy27atImVK1fSrVs3vL292bRpEydPnjSWHz9+PMOHD8fV1ZUePXpQXFzMli1bOHPmDKNGjSI2NpZJkybxwAMPMGHCBAICAjh8+DC//fYbr7zyCgEBATdemULUMJKohagBhg8fTk5ODi+99BJZWVlERESwcOFCQkNDr1h+5MiR6PV67r33XpYuXUr37t35888/mTBhAh988AE2NjaEhYXxzDPPXHcMtra2vPbaa6SlpeHg4EDHjh2ZN2/eFcu6uLiwZs0apk6dSm5uLkFBQXz00Uf07NkTgGeeeQadTsekSZN4+eWXcXR0JDIykpEjRwKg0+lYs2YNY8aM4cEHHyQvL4+6devSpUsXaWGL245GKaXMHYQQQgghrkweeCKEEEJYMEnUQgghhAWTRC2EEEJYMEnUQgghhAWTRC2EEEJYMEnUQgghhAWTRP0vPvvsM4KDg7G3t6ddu3Zs3rzZ3CFZhDVr1tC7d2/8/f3RaDQsWLDAZLlSirFjx+Ln54eDgwNdu3Zl//79JmWys7OJjY3FxcUFNzc3nn76afLz803K7Ny5k44dO2Jvb0+9evX48MMPL4vl559/JiwsDHt7eyIjI1m8eHGVf95baeLEibRp0wZnZ2e8vb3p06ePyXjUYHjW9ZAhQ/Dw8MDJyYl+/fpx4sQJkzLp6en06tULnU6Ht7c3L7/8sslwlgB///03rVq1ws7OjoYNGzJnzpzL4qmN/wMzZsygWbNmuLi44OLiQnR0NEuWLDEul/qtWu+//z4ajcZ4fzxIHd8QMw8KYpHmzZunbG1t1ddff63++ecf9eyzzyo3Nzd14sQJc4dmdosXL1avv/66+u233xSg5s+fb7L8/fffV66urmrBggVqx44d6v7771chISHq3LlzxjI9evRQzZs3Vxs3blRr165VDRs2VI899phxeU5OjvLx8VGxsbFq165d6ocfflAODg7qiy++MJZJSEhQVlZW6sMPP1S7d+9Wb7zxhrKxsVHJycnVXgfVpXv37mr27Nlq165dKikpSd17770qMDBQ5efnG8s8//zzql69emrlypVqy5Yt6o477lDt27c3Li8rK1NNmzZVXbt2Vdu3b1eLFy9Wnp6e6rXXXjOWOXTokNLpdGrUqFFq9+7davr06crKykotXbrUWKa2/g8sXLhQLVq0SO3bt0/t3btX/fe//1U2NjZq165dSimp36q0efNmFRwcrJo1a6ZGjBhhnC91XHmSqK+gbdu2asiQIcb35eXlyt/fX02cONGMUVmeSxO1Xq9Xvr6+atKkScZ5Z8+eVXZ2duqHH35QSim1e/duBajExERjmSVLliiNRqOOHTumlFLq888/V+7u7sZxh5VSasyYMapx48bG9/3791e9evUyiaddu3bqueeeq9LPaE5ZWVkKUKtXr1ZKGerSxsZG/fzzz8YyKSkpClAbNmxQShkOpLRarcrMzDSWmTFjhnJxcTHW5yuvvKKaNGlisq9HHnlEde/e3fj+dvofcHd3V1999ZXUbxXKy8tToaGhKj4+XnXu3NmYqKWOb4x0fV+ipKSErVu30rVrV+M8rVZL165d2bBhgxkjs3ypqalkZmaa1J2rqyvt2rUz1t2GDRtwc3MjKirKWKZr165otVo2bdpkLNOpUydsbW2NZbp3787evXs5c+aMsczF+7lQpjb9jnJycgCoU6cOAFu3bqW0tNTkc4eFhREYGGhSv5GRkfj4+BjLdO/endzcXP755x9jmavV3e3yP1BeXs68efMoKCggOjpa6rcKDRkyhF69el1WD1LHN0ae9X2JU6dOUV5ebvJHAuDj48OePXvMFFXNkJmZCXDFuruwLDMzE29vb5Pl1tbW1KlTx6RMSEjIZdu4sMzd3Z3MzMyr7qem0+v1jBw5kpiYGJo2bQoYPrutrS1ubm4mZS+t3yvVy4VlVyuTm5vLuXPnOHPmTK3+H0hOTiY6OpqioiKcnJyYP38+ERERJCUlSf1WgXnz5rFt2zYSExMvWyZ/wzdGErUQFmjIkCHs2rWLdevWmTuUWqdx48YkJSWRk5PDL7/8QlxcHKtXrzZ3WLXCkSNHGDFiBPHx8SbjnIubI13fl/D09MTKyuqyqxBPnDiBr6+vmaKqGS7Uz9XqztfXl6ysLJPlZWVlZGdnm5S50jYu3se/lakNv6OhQ4fy559/smrVKpPhHH19fSkpKeHs2bMm5S+t3xutOxcXFxwcHGr9/4CtrS0NGzakdevWTJw4kebNm/PJJ59I/VaBrVu3kpWVRatWrbC2tsba2prVq1czbdo0rK2t8fHxkTq+AZKoL2Fra0vr1q1ZuXKlcZ5er2flypVER0ebMTLLFxISgq+vr0nd5ebmsmnTJmPdRUdHc/bsWbZu3Wos89dff6HX62nXrp2xzJo1aygtLTWWiY+Pp3Hjxri7uxvLXLyfC2Vq8u9IKcXQoUOZP38+f/3112Xd/61bt8bGxsbkc+/du5f09HST+k1OTjY5GIqPj8fFxYWIiAhjmavV3e32P6DX6ykuLpb6rQJdunQhOTmZpKQk4xQVFUVsbKzxtdTxDTD31WyWaN68ecrOzk7NmTNH7d69Ww0aNEi5ubmZXIV4u8rLy1Pbt29X27dvV4CaMmWK2r59uzp8+LBSynB7lpubm/r999/Vzp071QMPPHDF27NatmypNm3apNatW6dCQ0NNbs86e/as8vHxUQMGDFC7du1S8+bNUzqd7rLbs6ytrdXkyZNVSkqKeuutt2r87VkvvPCCcnV1VX///bfKyMgwToWFhcYyzz//vAoMDFR//fWX2rJli4qOjlbR0dHG5RdubenWrZtKSkpSS5cuVV5eXle8teXll19WKSkp6rPPPrvirS218X/g1VdfVatXr1apqalq586d6tVXX1UajUYtX75cKSX1Wx0uvupbKanjGyGJ+l9Mnz5dBQYGKltbW9W2bVu1ceNGc4dkEVatWqWAy6a4uDillOEWrTfffFP5+PgoOzs71aVLF7V3716TbZw+fVo99thjysnJSbm4uKgnn3xS5eXlmZTZsWOH6tChg7Kzs1N169ZV77///mWx/PTTT6pRo0bK1tZWNWnSRC1atKjaPvetcKV6BdTs2bONZc6dO6cGDx6s3N3dlU6nU3379lUZGRkm20lLS1M9e/ZUDg4OytPTU7300kuqtLTUpMyqVatUixYtlK2trapfv77JPi6ojf8DTz31lAoKClK2trbKy8tLdenSxZiklZL6rQ6XJmqp48rTKKWUedryQgghhLgWOUcthBBCWDBJ1EIIIYQFk0QthBBCWDBJ1EIIIYQFk0QthBBCWDBJ1EIIIYQFk0R9FcXFxYwbN47i4mJzh1IrSf1WL6nf6id1XL2kfg3kPuqryM3NxdXVlZycHFxcXMwdTq0j9Vu9pH6rn9Rx9ZL6NZAWtRBCCGHBJFELIYQQFqzWj0ddVlbG9u3b8fHxQaut3HFJXl4eAMeOHSM3N7c6wrutSf1WL6nf6id1XL1qc/3q9XpOnDhBy5Ytsba+eiqu9eeoExMTadu2rbnDEEIIIS6zefNm2rRpc9Uytb5F7ePjAxgqw8/Pz8zRCCGEEJCRkUHbtm2NOepqan2ivtDd7efnR0BAgJmjEUIIISpczylZs15MtmbNGnr37o2/vz8ajYYFCxaYLFdKMXbsWPz8/HBwcKBr167s37/fPMEKIYQQZmDWRF1QUEDz5s357LPPrrj8ww8/ZNq0acycOZNNmzbh6OhI9+7dKSoqusWRCiGEEOZh1q7vnj170rNnzysuU0oxdepU3njjDR544AEAvv32W3x8fFiwYAGPPvrorQxVCCGEMAuLPUedmppKZmYmXbt2Nc5zdXWlXbt2bNiw4V8TdXFxscnj5i5c3i+EENejvLyc0tJSc4chajgbGxusrKyqZFsWm6gzMzMBLrsizsfHx7jsSiZOnMj48eOrNTYhRO2jlCIzM5OzZ8+aOxRRS7i5ueHr64tGo7mp7Vhsor5Rr732GqNGjTK+P3bsGBEREVWz8fIy+GsChHSGhl2qZptCCItwIUl7e3uj0+lu+stV3L6UUhQWFpKVlQVw07cGW2yi9vX1BeDEiRMmH/LEiRO0aNHiX9ezs7PDzs7O+L4qn2ZzauUneK7/BLZ9C4NWg3tQlW1bCGE+5eXlxiTt4eFh7nBELeDg4ABAVlYW3t7eN9UNbrHP+g4JCcHX15eVK1ca5+Xm5rJp0yaio6NveTwZOefosjaUHfr6cO4M/PQElMrV50LUBhfOSet0OjNHImqTC39PN3vNg1kTdX5+PklJSSQlJQGGC8iSkpJIT09Ho9EwcuRI3nnnHRYuXEhycjJPPPEE/v7+9OnT55bH6ufqQO/WIbxQMpIzuEBGEix+CWr3E1iFuK1Id7eoSlX192TWRL1lyxZatmxJy5YtARg1ahQtW7Zk7NixALzyyisMGzaMQYMG0aZNG/Lz81m6dCn29vZmiff1eyNw8ApiSMlQ9Ghh+3ewdY5ZYhFCCHF7MGuivvPOO1FKXTbNmTMHMByNTJgwgczMTIqKilixYgWNGjUyW7wOtlZ88mhLEjWRfFja3zBzyStwdKvZYhJCiKoWHBzM1KlTr7v833//jUajqfYr5ufMmYObm1u17sMSWew5akvVtK4rL3dvzMzy3ixXbaG8BH4aAPknzR2aEOI2o9ForjqNGzfuhrabmJjIoEGDrrt8+/btycjIwNXV9Yb2J67OYq/6tmTPdKjP6n0nGXVgEEscj1Mv9yj88iQMWABWUqVCiFsjIyPD+PrHH39k7Nix7N271zjPycnJ+FopRXl5+TXHPgbw8vKqVBy2trbGO3VE1ZMW9Q3QajV89HALrHWuDCwcQbFWB2lrYaU8aEUIcev4+voaJ1dXVzQajfH9nj17cHZ2ZsmSJbRu3Ro7OzvWrVvHwYMHeeCBB/Dx8cHJyYk2bdqwYsUKk+1e2vWt0Wj46quv6Nu3LzqdjtDQUBYuXGhcfmnX94Uu6mXLlhEeHo6TkxM9evQwObAoKytj+PDhuLm54eHhwZgxY4iLi6v0xcIzZsygQYMG2Nra0rhxY/73v/8ZlymlGDduHIGBgdjZ2eHv78/w4cONyz///HNCQ0Oxt7fHx8eHhx56qFL7vlUkUd8gX1d73n+wGQdVXV4sftYwc/00+GeBWeMSQlQNpRSFJWVmmVQV3k3y6quv8v7775OSkkKzZs3Iz8/n3nvvZeXKlWzfvp0ePXrQu3dv0tPTr7qd8ePH079/f3bu3Mm9995LbGws2dnZ/1q+sLCQyZMn87///Y81a9aQnp7O6NGjjcs/+OADvv/+e2bPnk1CQgK5ubmXjaB4LfPnz2fEiBG89NJL7Nq1i+eee44nn3ySVatWAfDrr7/y8ccf88UXX7B//34WLFhAZGQkYLiYefjw4UyYMIG9e/eydOlSOnXqVKn93yrST3sTejT15dE29ZiXCN/Z9+E/5Qtg8WgI7Qa2cj+mEDXZudJyIsYuM8u+d0/ojs62ar6eJ0yYwD333GN8X6dOHZo3b258//bbbzN//nwWLlzI0KFD/3U7AwcO5LHHHgPgvffeY9q0aWzevJkePXpcsXxpaSkzZ86kQYMGAAwdOpQJEyYYl0+fPp3XXnuNvn37AvDpp5+yePHiSn22yZMnM3DgQAYPHgwY7hzauHEjkydP5q677iI9PR1fX1+6du2KjY0NgYGBtG3bFoD09HQcHR257777cHZ2JigoyHgHkqWRFvVNevO+CEI8HXmroB8JLveiBsyXJC2EsBhRUVEm7/Pz8xk9ejTh4eG4ubnh5ORESkrKNVvUzZo1M752dHTExcXF+IjMK9HpdMYkDYbHaF4on5OTw4kTJ4xJE8DKyorWrVtX6rOlpKQQExNjMi8mJoaUlBQAHn74Yc6dO0f9+vV59tlnmT9/PmVlZQDcc889BAUFUb9+fQYMGMD3339PYWFhpfZ/q0iL+iY52lkz9ZEW9Juxntis/zD5mDsPyTUVQtR4DjZW7J7Q3Wz7riqOjo4m70ePHk18fDyTJ0+mYcOGODg48NBDD1FSUnLV7djY2Ji812g06PX6SpWvyi7961GvXj327t3LihUriI+PZ/DgwUyaNInVq1fj7OzMtm3b+Pvvv1m+fDljx45l3LhxJCYmWtwtYNKirgLN67nx4j2G+7vf+n0Xh08XwJHNkPiVmSMTQtwojUaDztbaLFN1PiEtISGBgQMH0rdvXyIjI/H19SUtLa3a9nclrq6u+Pj4kJiYaJxXXl7Otm3bKrWd8PBwEhISTOYlJCSYDMTk4OBA7969mTZtGn///TcbNmwgOTkZAGtra7p27cqHH37Izp07SUtL46+//rqJT1Y9pEVdRZ7v3IDV+06yOTWbSd8tZHrOMDSqHLzCILiDucMTQggAQkND+e233+jduzcajYY333zzqi3j6jJs2DAmTpxIw4YNCQsLY/r06Zw5c6ZSBykvv/wy/fv3p2XLlnTt2pU//viD3377zXgV+5w5cygvL6ddu3bodDq+++47HBwcCAoK4s8//+TQoUN06tQJd3d3Fi9ejF6vp3HjxtX1kW+YtKiriJVWw8ePtMDZ3po/M5z5x7MHhN8Pfi3MHZoQQhhNmTIFd3d32rdvT+/evenevTutWrW65XGMGTOGxx57jCeeeILo6GicnJzo3r17pR4R3adPHz755BMmT55MkyZN+OKLL5g9ezZ33nknYBgP+ssvvyQmJoZmzZqxYsUK/vjjDzw8PHBzc+O3337j7rvvJjw8nJkzZ/LDDz/QpEmTavrEN06jbvVJg1vs6NGj1KtXjyNHjhAQEFDt+/tjx3GG/bAdW00Z3w/qQJsQGTJPCEtXVFREamoqISEhZhtL4Han1+sJDw+nf//+vP322+YOp0pc7e+qMrlJWtRVrHdzfx5sVZcSZc3IH3eQW1RqGGFr/4prryyEELeJw4cP8+WXX7Jv3z6Sk5N54YUXSE1N5fHHHzd3aBZHEnU1GH9/E+rVceDY2XOMnZ8MvzwF3/eTkbaEEOI8rVbLnDlzaNOmDTExMSQnJ7NixQrCw8PNHZrFkURdDZztbZj6SEustBoW7MjgH32gYcHil2WkLSGEwHDrVEJCAjk5OeTm5rJ+/XqLfTKYuUmiriatg9wZfncoAI/ujqawfo/zI209AQWnzBydEEKImkISdTUaclcDWge5k1dcznN5z6DqNIQLI22Vl5k7PCGEEDWAJOpqZG2lZeojLXCys2btkRK+D34PbBwhdQ38NeHaGxBCCHHbk0RdzerV0fF2H8N9eW9tLCe1w4eGBQmfwO7fzRiZEEKImkAS9S3Qp0Vd7m/uT7leEbepLiVthxgWLBgMJ/defWUhhBC3NUnUt4BGo+HtPk2p6+ZAenYhb+T3g+COUJIPP/4HivPMHaIQQggLJYn6FnF1sOHjR1qg1cBP2zJZHjERnP3h1D5Dy7p2PyBOCGHB7rzzTkaOHGl8HxwczNSpU6+6jkajYcGCBTe976raztWMGzeOFi1aVOs+qpMk6luobUgdBt/ZEIDRi49z8t4vQWsDKQsN56yFEKISevfuTY8ePa64bO3atWg0Gnbu3Fnp7SYmJjJo0KCbDc/EvyXLjIwMevbsWaX7qm0kUd9iI7qG0jzAldyiMoausULf4wNw9IZ6ba+9shBCXOTpp58mPj6eo0ePXrZs9uzZREVF0axZs0pv18vLC51OVxUhXpOvry92dna3ZF81lSTqW8zGSsvUR1uis7ViU2o2XxR0hqGbIai9uUMTQtQw9913H15eXsyZM8dkfn5+Pj///DNPP/00p0+f5rHHHqNu3brodDoiIyP54YcfrrrdS7u+9+/fT6dOnbC3tyciIoL4+PjL1hkzZgyNGjVCp9NRv3593nzzTUpLSwHDcJPjx49nx44daDQaNBqNMeZLu76Tk5O5++67cXBwwMPDg0GDBpGfn29cPnDgQPr06cPkyZPx8/PDw8ODIUOGGPd1PfR6PRMmTCAgIAA7OztatGjB0qVLjctLSkoYOnQofn5+2NvbExQUxMSJEwFQSjFu3DgCAwOxs7PD39+f4cOHX/e+b4SMR20GIZ6OjOvdhFd+3clH8fvoEBpD5IXBU9I3gY0D+FX+KFgIUQ1KCiq/jpUdWJ3/ei0vg/Ji0GgN/9vX2q6t43XvxtramieeeII5c+bw+uuvG8dy/vnnnykvL+exxx4jPz+f1q1bM2bMGFxcXFi0aBEDBgygQYMGtG177Z48vV7Pgw8+iI+PD5s2bSInJ8fkfPYFzs7OzJkzB39/f5KTk3n22WdxdnbmlVde4ZFHHmHXrl0sXbrUOFa0q6vrZdsoKCige/fuREdHk5iYSFZWFs888wxDhw41ORhZtWoVfn5+rFq1igMHDvDII4/QokULnn322euqt08++YSPPvqIL774gpYtW/L1119z//33888//xAaGsq0adNYuHAhP/30E4GBgRw5coQjR44A8Ouvv/Lxxx8zb948mjRpQmZmJjt27Liu/d4oi07U5eXljBs3ju+++47MzEz8/f0ZOHAgb7zxRqUGF7dED0cFsGpvFkt2ZTJi3nb+HN4BXVYS/K8v2NjDU8vBs6G5wxRCvOdf+XUengNN+hpe7/kDfh4IQR3gyUUVZaZGQuHpy9cdl1OpXT311FNMmjSJ1atXG8dhnj17Nv369cPV1RVXV1dGjx5tLD9s2DCWLVvGTz/9dF2JesWKFezZs4dly5bh72+oi/fee++y88pvvPGG8XVwcDCjR49m3rx5vPLKKzg4OODk5IS1tTW+vr7/uq+5c+dSVFTEt99+i6Oj4YDl008/pXfv3nzwwQf4+PgA4O7uzqeffoqVlRVhYWH06tWLlStXXneinjx5MmPGjOHRRx8F4IMPPmDVqlVMnTqVzz77jPT0dEJDQ+nQoQMajYagoCDjuunp6fj6+tK1a1dsbGwIDAy8rnq8GRbd9f3BBx8wY8YMPv30U1JSUvjggw/48MMPmT59urlDu2kajYaJD0bi62LPoVMFvP1nCniGglcj8G0GLjfw5SCEuO2EhYXRvn17vv76awAOHDjA2rVrefrppwFDg+ftt98mMjKSOnXq4OTkxLJly0hPT7+u7aekpFCvXj1jkgaIjo6+rNyPP/5ITEwMvr6+ODk58cYbb1z3Pi7eV/PmzY1JGiAmJga9Xs/evRXPnGjSpAlWVlbG935+fmRlZV3XPnJzczl+/DgxMTEm82NiYkhJSQEM3etJSUk0btyY4cOHs3z5cmO5hx9+mHPnzlG/fn2effZZ5s+fT1lZ9T4S2qJb1OvXr+eBBx6gV69egOEo7YcffmDz5s1mjqxquOlsmfJIc2K/2sQPm9Pp3MiLHgPmg7WDoVUthDC//x6v/DpWF10cFdbbsA3NJe2ikck3F9dFnn76aYYNG8Znn33G7NmzadCgAZ07dwZg0qRJfPLJJ0ydOpXIyEgcHR0ZOXIkJSUlVbb/DRs2EBsby/jx4+nevTuurq7MmzePjz76qMr2cTEbGxuT9xqNBr1eX2Xbb9WqFampqSxZsoQVK1bQv39/unbtyi+//EK9evXYu3cvK1asID4+nsGDBxt7NC6Nq6pYdIu6ffv2rFy5kn379gGwY8cO1q1bV6su5W/fwJNBneoDMPrnHezJsapI0krB+ulwtnJHpUKIKmTrWPnJ6qI2kJW1Yd7F56evtt0b0L9/f7RaLXPnzuXbb7/lqaeeMp4eTEhI4IEHHuA///kPzZs3p379+sbv1OsRHh7OkSNHyMjIMM7buHGjSZn169cTFBTE66+/TlRUFKGhoRw+fNj049raUl5efs197dixg4KCivP3CQkJaLVaGjdufN0xX42Liwv+/v4kJCSYzE9ISCAiIsKk3COPPMKXX37Jjz/+yK+//kp2djYADg4O9O7dm2nTpvH333+zYcMGkpOr7sDrUhbdon711VfJzc0lLCwMKysrysvLeffdd4mNjf3XdYqLiykuLja+z8uz/Kd+vXRPY5LSz7IpNZunZicyf0gMPi72hiQd/yYk/h88uVi6w4UQV+Tk5MQjjzzCa6+9Rm5uLgMHDjQuCw0N5ZdffmH9+vW4u7szZcoUTpw4YZKUrqZr1640atSIuLg4Jk2aRG5uLq+//rpJmdDQUNLT05k3bx5t2rRh0aJFzJ8/36RMcHAwqampJCUlERAQgLOz82W3ZcXGxvLWW28RFxfHuHHjOHnyJMOGDWPAgAHG89NV4eWXX+att96iQYMGtGjRgtmzZ5OUlMT3338PwJQpU/Dz86Nly5ZotVp+/vlnfH19cXNzY86cOZSXl9OuXTt0Oh3fffcdDg4OJuexq5pFt6h/+uknvv/+e+bOncu2bdv45ptvmDx5Mt98882/rjNx4kTjBRSurq7X/cdoTrbWWmYNiKK+lyPHc4p4+ptECorLoGk/cAuCM6nwzf2Qf33nYIQQt5+nn36aM2fO0L17d5PzyW+88QatWrWie/fu3Hnnnfj6+tKnT5/r3q5Wq2X+/PmcO3eOtm3b8swzz/Duu++alLn//vt58cUXGTp0KC1atGD9+vW8+eabJmX69etHjx49uOuuu/Dy8rriLWI6nY5ly5aRnZ1NmzZteOihh+jSpQuffvpp5SrjGoYPH86oUaN46aWXiIyMZOnSpSxcuJDQ0FDAcAX7hx9+SFRUFG3atCEtLY3Fixej1Wpxc3Pjyy+/JCYmhmbNmrFixQr++OMPPDw8qjTGi2mUstxnV9arV49XX32VIUOGGOe98847fPfdd+zZs+eK61zaoj527BgREREcOXKEgICAK65jKdJPF9L38wROF5TQNdybLwZEYZWTDrPvNYxj7R0BcX+CY/X9QQhxOyoqKiI1NZWQkBDs7eX6EFE1rvZ3dfToUerVq3dducmiW9SFhYVotaYhWllZXfWiATs7O1xcXIyTs7NzdYdZZQI9dMx6Igpbay0rUrJ4+8/d4B4EcQvByReydsP/+sC5M+YOVQghxC1i0Ym6d+/evPvuuyxatIi0tDTmz5/PlClT6Nu3r7lDqzatg9z5uH8LAOasT+Prdang0QDi/gBHL8jcCd/1g6Jc8wYqhBDilrDoRD19+nQeeughBg8eTHh4OKNHj+a5557j7bffNndo1apXMz9e7RkGwNuLdrP8n0zD/dVP/A4O7nBsK3z/MBTnX2NLQgghajqLTtTOzs5MnTqVw4cPc+7cOQ4ePMg777yDra2tuUOrds91qs9jbQNRCkbMS2Ln0bPg0wQGLAA7VziyEX54FErPmTtUIYQQ1ciiE/XtTKPR8PYDTejUyItzpeU8NWcLR88Ugn8LGPAb2DpD2lqYFwtlxdfcnhBCiJpJErUFs7bS8tnjLQnzdeZUfjFPzUkkt6gUAqIg9mew0cHBlfDLU4aHowghbkpVPt1KiKr6e7LoB54IcLa34euBbej7eQL7TuQz+LttzH6yDTZB0fDYPJj3uOHh/zV8kBIhzMnW1hatVsvx48fx8vLC1ta2xg/8I8xHKUVJSQknT55Eq9Xe9Olai76PuipU5l41S7brWA79v9hAYUk5j0TV4/1+kYYvkoLTcl+1EFWgpKSEjIwMCgsLzR2KqCV0Oh1+fn5XTNSVyU3Soq4hmtZ15dPHW/LMN1v4ccsRAj10DLmroWmSzj0OW+dA51dBK2c1hKgMW1tbAgMDKSsru+YzqYW4FisrK6ytraukZ0YSdQ1yd5gP4+5vwtjf/2HSsr3Uq6Pj/ubnHxVYVmJ4zOjp/Ybz1Xe/fvWNCSEuo9FosLGxqbZRkIS4EdLsqmGeiA7m6Q4hgGG0rS1phtFcsLaFzq9AnfrQaoAZIxRCCFGVJFHXQP+9N5zuTXwoKdPz7LdbSDt1fki4Zv1h8EZwCzRvgEIIIaqMJOoayEqrYeojLWke4MqZwlKenJPImYLzg8BbXzRsXMofsHqSeYIUQghRJSRR11AOtlZ8FdeGum4OpJ4qYND/tlBUetEFMKcPwk9xsOodWPex+QIVQghxUyRR12BeznbMebINzvbWJKad4ZVfdqLXn7/bzqMB3P2G4fWKcTDrTtg4E/JPmitcIYQQN0ASdQ0X6uPMzP+0xlqrYeGO40yJ31exsOMouPtN0FjB8e2wdAx81NgwoEfyL1Ai94sKIYSlk0RdC8Q09OS9ByMB+HTVAX5KPFKxsNNoeGkv9PwQ/FuBKof9y+HXp2FyI1gwGA6tBnl0ohBCWCRJ1LVE/6h6DL2rIQD/nZ/Muv2nKhY6eUG752DQKhiSCB1Hg2sglORB0vfw7f0wtSnsW26m6IUQQvwbSdS1yEvdGnF/c3/K9IoXvtvKvhN5lxfyagRd3oQRO+DJJdAqzjBsZu4xcPGrKHcmDfIyb1nsQgghrkwSdS2i0WiY9HAz2gS7k1dcxpOzE8nKK7pyYa0WgtrD/dNg9D6I/QV8mlYsXzURpoTDpi9uTfBCCCGuSBJ1LWNnbcWsAVGEeDpy7Ow5nvlmC4UlZVdfycYeQu+pGIFLKcg/AUoP/i0ryp3cCwf/Ar08B1kIIW4VSdS1kLujLbMHtsFdZ8POozmMmJdEaXklLhbTaOCJBTA8CQLaVMzf+Dn8ry983ASWvwGZu6o6dCGEEJeQQTlqqWBPR758IorHv9pE/O4T3Dnpb57uEMKjbeuhs73OX3udENP3DnXAwR3yMmD9dMOk8wR7V7BzPj+5XPTa2dBSD2pvWL84D44ngYMb+EZW5ccVQohaS8ajruXid5/gtd+SOZVfDICbzoYn7ggirn0wHk5211j7CspKDLd37ZwH+5ZBecnVy9/zNsQMN7w+thW+vBtc68GLF7XGv+4JJ/eYJnp7F3D2Mzy33C0Q3ILAPQgcvSq66IUQooaS8aiF0T0RPnQM9eS3bceYteYgaacLmfbXAWatPUT/qHo806E+gR6669+gtS2E32eYinLg7BFDS7k4D4pzL3p9frr4HDeAZ2Nw9jGdV3gazmUbpmvu396QuO94AaKeMswrKYSsFMN8J6/r/yxCCFEDSIv6NlKuVyz/J5OZqw+y42gOAFoN9Grmz3Od6tO0rqt5AsvNMCT9i5N90VnIPQ5nDsPZdMOUeww4/+fa4wO443nD66Nb4au7wdkfXkqp2O6Gz0BfZtoq13lIi1wIcW1lxYZbVPMyDN9FeZmGRzM36l4lm5cWtbgiK62GnpF+9Gjqy8ZD2cxcfZDV+07yx47j/LHjOB1DPXmuUwNiGnqguZXJzMXP9B7uf1NWYkjWZ9NNz5+X5IFLXUMivtjGGZBzxHSejc7Q9e7ib1jHxc/w2tnf8NM92NDtbk5KGQ5U8k9CwUkoyIKiXEPXv3cT6TUQoiqcOgDZB88n4YzzCTnjfHI+bujpu1Tkw1WWqCtDWtS3ud3Hc5m15iB/7Myg/PyAHpF1XXmuc316NPHF2qoG3Rig1xvuD79g1XuQfaiiRZ6Xce1t9PzQ8BQ3MHSnr5kMvk2hw4sVZQpOgb0bWN3gce6Zw5C+0XBR3YV/er0evrwL8rMMyVlf+u/r6zzBJwK8I6BFLPg1u7E4hKgJ9OVQXmroHdOXGd4bX5+flN70vb0r1KlvWL84z/Co5PwThoc8aa0M839+Ev757er7trIzHMw7n5+C2kPbZ6vkY0mLWly3CH8Xpj7akpe6Neb/1qXyY+IRko/lMHTudgLr6Hi2U30ebh2AvY2VuUO9Nu0lBxV3/df0fWlRRYs8L8PwOve44Sj6wmsX/4ryJ/fCrl8g56hpop51p6G8k8/51rhfRetc6c+3hM8n3Auv+8yERt0M66dvhPmDIKRTRaLWag2t/4uP4u1cwdETnLzB1slw9J+dCoWnIHWNYQrpXJGoD66CzbOgYVdo83SVVKkQlykvNfytlpeCV+OK+Qf/MvxvlRRCacH5n4VQUnD+5yXzSwuhaT/oMtawfmE2fNLckGhfO1qRUH99Gv6ZX7kYm/aDh742vLbRwZ4/Df+bBSfB2dcw3ysM/JobetOcfSv+l539KpKzg7tFnCqz+ER97NgxxowZw5IlSygsLKRhw4bMnj2bqKgoc4dWq9Sro2Pc/U0Y3iWUbzek8c36NNKzC3lzwS6mxu9jYPtgBkQH4aazNXeoN87G3nCOyaPB9ZX3aQLd3jGc175Arze0qJW+orvseuSfqHhdJ8SQYC+90K7/t4YvFUcvw2Rjf/l2SgoMV8hnpcCJ3eDfomLZ0UTYu9jQmriQqMtL4asu4NkIvMMNXec+EYbu/5v5AtLrDdcTlBZC6bnzX8bnKr6AL7zWWoOtI9g4gq0OfJuBnZNhG2UloNHeeM9EbadURT2W5BsSXEnB+WR3PuGV5FfUub4c6t8Fge0M659Jg4RPDL0/Xd+q2O6y1+HUfkOvzcUt1Quvy0sNy/RlUF4GZUXQ7nm4c0zFdj+NMhxIvpZesd2ET+DQ35X7jPlZFa+1Voa/KTB8lguJWnuFvw+N1jDfOFld8t7adLv3TTX0YNk5V8y/c0zFZ7JwFv0fcubMGWJiYrjrrrtYsmQJXl5e7N+/H3d3d3OHVmvVcbRlZNdGPNepAT9tOcKXaw9x9Mw5Porfx4zVB3m0TSBPdwyhrpuDuUOtfp6hhuliWi3897jhyDz3WMWFJhcmrbXhHLKjFzh6V7SI3QIrtlGvLcQtvHx/wR2uHZOtI9RtbZguFXafIUl7NKyYd/ogZOwwTCbbcTYkbp8IQ4ui7JxpS+e+qRXn6ld/CDt+gDbPQvRgw7yTKTCj/bXjvdTz6yruoV//Cfz1DrR+EnpPNcwryoVvehs+p62j4cDF+FNn6Fmw0VUcxChlOGgKv7/iOofj2yF1reHgpHEPwzx9OaydApwvf2E91CWvL34wkAaaP2qoJzA84Gf3AnAPgZaxFcXWTTVceKTRGNbRnF/XeCB00evSIkOijegLAed/h0c2Gx4g5B4CD170yN6pkZdfY3Et1vYVibrgNGz52jAAz8WJ+vB6OL6tctstOlvx2njg5Whapl47sHYw/J4u/b1dOFAz/jw/XXwHiK0zDNtWkXQv6P0J3PdxRQLWWF3ee3YtreMqV97CWHSi/uCDD6hXrx6zZ882zgsJCbnKGqKqONhaEdc+mNh2gSxKzmDm6kOkZOTydUIq325I4/7m/jzXuQGNfZ2vvbHaRqs1fMFcepuZuflEGKaLudaFx+ZB1m5DCzxrN5zaZ7gA7+hmw3Ql97xdkajPnTWc67+4V8Dm/IGaxur8F7JDxZevra7ifXmpaYvQ7qIL9UoKDD+tL+o5KMmHjKTKf3bfZhWJOn0jxL8JTR+qSNRKwap3Kr/dgKiKRJ2VAmsmGXpDLk7Ua6dAcU7ltlunQUWiLimAI5ugON+0zMX1Yu1w/uDlooMVW0fDa1udoayVremDhFz84c7XDN23F+s02tDNbGVTkfysbEBrY0iSxtfWht4OawfDgefF2339+OWf6dJTTZWl1V65t+vSA4LbkEVfTBYREUH37t05evQoq1evpm7dugwePJhnn73+k/lyMVnVUEqxdv8pZq4+yPqDFedR72zsxcD2wXQK9UKrNf+5HHEdykrg9IHzyfsfw3nxC62fCwm2ZayhdQ6GVnnBSXANMExg6PrWlxm+1G+0C72sxJCYtVYV+yo9Z2gNX9y9e+G8prHbt9DQA4DG0AWq0RgS0oXzpQdWQvLPhiTb5pmKeP8cWVH+4nU1WtP3FyhlqAefJob3x7YZehY8QqHdoIpyS/9rOBi50Do3/uT86/M/wZBQbXWGHoB6bQ3zCk5B+gbDRYJB0RXbvZBMbXQV3cCi1qhMbrLoRG1vbziiHDVqFA8//DCJiYmMGDGCmTNnEhd35a6M4uJiiouLje+PHTtGRESEJOoqtOPIWWatOcSSXRmcv1Cc+p6OPBEdRL/WATjb25g3QCGEsHC1JlHb2toSFRXF+vXrjfOGDx9OYmIiGzZsuOI648aNY/z48ZfNl0Rd9Q6fLuCb9Yf5ecsR8ooNI3Q52VnzUOsAnogOor6Xk5kjFEIIy1SZRG3RN8n6+fkREWF6zi08PJz09PR/WQNee+01cnJyjNPu3burO8zbVpCHI2N7R7Dxv114+4EmNPByJL+4jDnr07j7o9XEfb2ZVXuy0Ost9lhQCCEs3g1dTHbkyBE0Go3xKGDz5s3MnTuXiIgIBg0adI21r19MTAx79+41mbdv3z6CgoL+ZQ2ws7PDzq5isInc3Nwqi0dcmaOdNQOig/nPHUGsO3CKb9ansXJPFqv3nWT1vpMEe+h4IjqYh6ICcJFucSGEqJQbalE//vjjrFq1CoDMzEzuueceNm/ezOuvv86ECROqLLgXX3yRjRs38t5773HgwAHmzp3LrFmzGDJkSJXtQ1QdjUZDx1Avvoprw+rRd/FMhxCc7a1JO13IhD93E/3eSsb+vosDWfnX3pgQQgjgBs9Ru7u7s3HjRho3bsy0adP48ccfSUhIYPny5Tz//PMcOnSoygL8888/ee2119i/fz8hISGMGjVKrvquQQqKy5i//RjfrE9j/0UJumOoJwPbB3NXY2+5WlwIcdup9keIlpaWGruXV6xYwf333w9AWFgYGRnX+aSm63Tfffdx3333Vek2xa3jaGfNf+4IIrZdIBsOnmb2+jRWpJxg7f5TrN1/isA6Op6IDuLhqHq4Oki3uBBCXOqGur6bNGnCzJkzWbt2LfHx8fToYXiowPHjx/Hw8LjG2uJ2pNFoaN/Qky+fiGLNy3cxqFN9XOytSc8u5J1FKURPXMkbC5LZfyLP3KEKIYRFuaGu77///pu+ffuSm5tLXFwcX39tePj5f//7X/bs2cNvv11jRJJbSLq+LVdhSRkLth/nm/Vp7L0oQcc09CAuOpj2DT1xsrPoh+cJIcQNuSX3UZeXl5Obm2vy3O20tDR0Oh3e3t43sslqIYna8iml2HgomznrU4nffYKL7+bydLIj2ENHkIej4aenI0F1dAR7OOKqk65yIUTNVO3nqM+dO4dSypikDx8+zPz58wkPD6d791s/qLao2TQaDdENPIhu4MHRM4V8tzGd37YdJSuvmFP5hmnL4TOXreems6lI4B7nE7in4bWHoy0aCxieTgghbtYNtai7devGgw8+yPPPP8/Zs2cJCwvDxsaGU6dOMWXKFF544YXqiPWGSIu65so5V0r66ULSTheQnl1I2qkCDp9/n5VXfNV1neysCfIwtLwDPXQXtcod8Xa2kyvNhRBmVe0t6m3btvHxxx8D8Msvv+Dj48P27dv59ddfGTt2rEUlalFzuTrYEBngSmSA62XLCkvKOHy6kMOnLyTvitfHc86RX1zGP8dz+ef45Q+8cdfZ8EzH+jwZE4zOVs6BCyEs2w19SxUWFuLsbBjecPny5Tz44INotVruuOMODh8+XKUBCnElOltrwv1cCPdzuWxZUWk5R88UmiTwCz+PnjnHmcJSJi3by+yEVIbc1ZDH2wViZy2jEwkhLNMNJeqGDRuyYMEC+vbty7Jly3jxxRcByMrKwsXl8i9OIW4lexsrGno709D78rGyS8v1LE7OYEr8Pg6fLmT8H7v5cs0hRnQNpV+rAKytLPrx90KI29ANfSuNHTuW0aNHExwcTNu2bYmONoyhunz5clq2bFmlAQpRlWystDzQoi4rRnVm4oOR+LrYczyniDG/JnPPx2tYuOO4DCIihLAoN3x7VmZmJhkZGTRv3hyt1pDvN2/ejIuLC2FhYVUa5M2Qi8nE1RSVlvP9pnQ+X3WA0wUlAIT5OjO6W2O6hHvLleNCiGpxS8ejPnr0KIDFJkFJ1OJ6FBSXMTshlS/WHCKvyDC2dot6brzSvTHtG3qaOTohRG1T7eNR6/V6JkyYgKurK0FBQQQFBeHm5sbbb7+NXq+/oaCFMCdHO2uG3h3K2lfu4oU7G+BgY0XSkbM8/tUmHv9yI9vSL7+PWwghboUbupjs9ddf5//+7/94//33iYmJAWDdunWMGzeOoqIi3n333SoNUohbxU1ny5geYTwZE8znqw4yd1M66w+e5sHP19M13JuXujW+4pXmQghRXW6o69vf35+ZM2caR8264Pfff2fw4MEcO3asygK8WdL1LW7G0TOFTFu5n1+2HjU+2rR3c39e7BpKfS8n8wYnhKixqr3rOzs7+4oXjIWFhZGdnX0jmxTCIgW46/jwoebEj+rMfc38APhjx3Hu+XgNY37ZybGz58wcoRCitruhRN28eXM+/fTTy+Z/+umnNGvW7KaDEsLSNPBy4tPHW7FoeAe6hHlTrlf8uOUId036m3EL/+HkNR5pKoQQN+qGur5Xr15Nr169CAwMNN5DvWHDBo4cOcLixYvp2LFjlQd6o6TrW1SHrYfPMHnZXjYcOg2Ag40VA2OC6d7El9JyPSVlekou/Dw/lZZfNO9flhWX6SktV5SUlRvLlZYprLQaBkQHcW+kn5k/uRCiKtyS27OOHz/OZ599xp49ewAIDw9n0KBBvPPOO8yaNetGNlktJFGL6pRw4BSTlu0l6cjZW7K/Pi38GX9/UxniU4ga7pbeR32xHTt20KpVK8rLy6tqkzdNErWobkopVqRkMePvA5zILcbWWoutlRYbaw22VlpsrbXYWGmxs9ZWLDs/3/aieReXvTDf7vz75GM5fLH6IHoFvi72THq4GR1Dvcz90YUQN6jaR88SQlTQaDTcE+HDPRE+1baPeyP9uCfCh5d+2kHqqQIG/N9mnogO4tWeYTICmBC1nIxAIEQN0SrQnUXDO/BEdBAA3244TK9p6+RhLELUcpKohahBdLbWTHigKd8+1RZfF3tSTxXw0Iz1fLR8LyVl8lRAIWqjSvWZPfjgg1ddfvbs2ZuJRQhxnTo18mLZyE6MXbiL35OOM/2vA/y1J4uPH2lBI5/Lh/cUQtRclUrUrq6u11z+xBNP3FRAQojr46qz4ZNHW3JPhA9vLNjFP8dzuW/6Ol7u1pinO4Sg1crIX0LUBlV61bclkqu+xe0gK7eIMb/uZNXekwC0C6nD5IebU6+OzsyRCSGupNofISqEsCzeLvZ8PbANEx+MRGdrxabUbHp+spafEo9Qy4/Fhaj1alSifv/999FoNIwcOdLcoQhhcTQaDY+1DWTJiI5EBbmTX1zGK7/u5Nlvt8ojToWowWpMok5MTOSLL76QZ4kLcQ1BHo78+Fw0r/YMw9ZKy4qUE3SfuoaluzLNHZoQ4gbUiESdn59PbGwsX375Je7u7uYORwiLZ6XV8HznBvw+NIYwX2eyC0p4/rutjPopidyiUnOHJ4SohBqRqIcMGUKvXr3o2rXrNcsWFxeTm5trnPLy8m5BhEJYpnA/F34fGsPgOxug1cBv247R4+M1JBw4Ze7QhBDXyeIT9bx589i2bRsTJ068rvITJ07E1dXVOEVERFRzhEJYNjtrK17pEcZPz0UT5KHjeE4RsV9tYvwf/1BUajnP5RdCXJlFJ+ojR44wYsQIvv/+e+zt7a9rnddee42cnBzjtHv37mqOUoiaISq4DouHdyS2XSAAsxPS6DVtLVsPn5Erw4WwYBZ9H/WCBQvo27cvVlZWxnnl5eVoNBq0Wi3FxcUmy65E7qMW4nKr9mYx5pedZJ2/Gtzf1Z6OoV50bORJTANP3B1tzRyhELWb2Ya5rGp5eXkcPnzYZN6TTz5JWFgYY8aMoWnTptfchiRqIa7sbGEJE/7YzZ/JGSbPCddoILKuKx1DPenQ0IvWQe7YWlt055sQNU6tGebS2dn5smTs6OiIh4fHdSVpIcS/c9PZMuWRFrzbN5LNadms3XeSdQdOsSczj51Hc9h5NIfPVh1EZ2vFHfU96BjqScdQTxp4OaHRyONJhbhVLDpRCyGqn4OtFZ0bedG5kRcAJ3KLWLf/FOsOnGLt/pOcyi/hrz1Z/LUnCwA/V3tDazvUiw4NPakj3eRCVCuL7vquCtL1LcSN0+sVezLzWHfgJGv3n2JTavZl3eRN/V3pcL613TrIHTvrq183IoSoReeoq4IkaiGqTlFpOYlp2azdf4o1+06yJ9P0OQUONla0q1+HjqFedAr1pKG3dJMLcSWSqC8iiVqI6pOVV0TCgVOs3XeKtQdOXfZM8UY+TsS1D6Zvy7robOVMmxAXSKK+iCRqIW4NpRR7T+QZk/amQ6cpPt9N7mJvTf+oejwRHUyghwy9KYQk6otIohbCPHKLSvlly1G+3ZBG2ulCwHBOu0uYN3Htg+nQ0FO6xcVtq9bcniWEqLlc7G14qkMIA9sHs3rfSeasT2P1vpOsSMliRUoWDb2diIsO4sFWATjayVeREP9GWtRCiFvm4Ml8/rfhML9sPUp+cRkAznbWPBxVjyeigwj2dDRzhELcGtL1fRFJ1EJYnryiUn7bdoxv1qdx6FQBYOgWv7ORFwNjQujY0BOtVrrFRe0lXd9CCIvmbG9DXPtgBtwRxNoDp5iTkMqqvSeNU31PR+LaB9OvdQBO0i0ubnPSohZCWIS0UwV8u+EwP285Qt75bnEnO2seah3AE9FB1PdyMnOEQlQd6fq+iCRqIWqWguIyftt2lDnr0zh4ssA4v3MjLwa2D6ZzIy/pFhc1nnR9CyFqLEc7awZEB/OfO4JYd+AU36xPY+WeLFbvO8nqfScJ9tDxeLtAGvk44+Nij6+LPW46G7nVS9RakqiFEBZJo9EYxsgO9SL9dCHfbkjjxy1HSDtdyHuL95iUtbXW4uNih6+LPT7nJ18Xe3xc7fFxtsPX1TDP3kaeQy5qHknUQgiLF+ih4437InjxnkbM336MlSknyMwt5kRuEdkFJZSU6TmSfY4j2eeuuh1XB5srJvCKxG6Hp6OddK0LiyKJWghRYzjaWfOfO4L4zx1BxnnFZeVknU/amblFZOYUkZVXTGaO4X3W+flFpXpyzpWSc66UvSfy/nUfAe4OvH5vOD2a+kp3urAIkqiFEDWanbUV9eroqFfn358hrpQi91wZmblFxoR+IqeIE3lFZOYYkvyJ3CJO5hdz9Mw5Xvh+Gx1DPRl3fxMayNXmwswkUQshaj2NRoOrzgZXnQ2NfZ3/tVxhSRmfrzrIrDWHWLv/FD2mruHpDvUZdndDecypMButuQMQQghLobO1ZnT3xix7sRN3NvaitFwxc/VBuk5ZzZ87j1PL72YVFkoStRBCXCLE05HZA9vw5RNRBLg7kJFTxNC524n9ahP7r3J+W4jqIIlaCCGuQKPRcE+EDytGdWZEl1BsrbWsP3ianp+s5d1Fu42DighR3SRRCyHEVdjbWPHiPY1Y8WJnuob7UKZXfLk2lbsn/83vScekO1xUO0nUQghxHQI9dHwVF8XsgW0I8tCRlVfMiHlJPDJrI3syc80dnqjFJFELIUQl3BXmzbKRnXjpnkbY22jZnJpNr2nrGP/HP+QWlZo7PFELSaIWQohKsrexYliXUFaM6kz3Jj6U6xWzE9K4e/Jqft16VLrDRZWSRC2EEDcowF3HFwOi+OapttT3dORUfjEv/byDh2du4J/jOeYOT9QSkqiFEOImdW7kxZKRHXmlR2McbKzYcvgMvaevY+zvu8gplO5wcXMsOlFPnDiRNm3a4OzsjLe3N3369GHv3r3mDksIIS5jZ23F4DsbsvKlzvRq5odewbcbDnP3R3/zU+IR9HrpDhc3xqIT9erVqxkyZAgbN24kPj6e0tJSunXrRkFBwbVXFkIIM/B3c+Czx1vx/TPtaOjtxOmCEl75dSf9Zq5nxe4TFJeVmztEUcNoVA266uHkyZN4e3uzevVqOnXqdF3rHD16lHr16nHkyBECAgKqOUIhhKhQUqZnzvpUPlmxn4ISQ4J2trPmniY+9Ir0o0OoJ3bWMkb27agyualGPWU+J8dwcUadOnXMHIkQQlybrbWWQZ0a8ECLusxcfZDFyRmcyC3mt23H+G3bMZztrbknwof7mvnRoaEXttYW3ckpzKTGtKj1ej33338/Z8+eZd26df9arri4mOLiYuP7Y8eOERERIS1qIYTZ6fWKrelnWLQzg8XJGWTlVXxXOdtb0y3Cl17NfCVp3wYq06KuMYn6hRdeYMmSJaxbt+6qH2rcuHGMHz/+svmSqIUQluRqSdvF3pp7Iny5r5kfMQ09JWnXQrUuUQ8dOpTff/+dNWvWEBISctWy0qIWQtQ0er1iy+EzLE6+ctLu1sSXXpGStGuTWpOolVIMGzaM+fPn8/fffxMaGlrpbcjFZEKImuRC0l608zhLdmVelrS7N/Hl3mZ+xDSQpF2T1ZpEPXjwYObOncvvv/9O48aNjfNdXV1xcHC4rm1IohZC1FTlesWWtGxDS3tXJicvStquDjZ0i/CRpF1D1ZpErdForjh/9uzZDBw48Lq2IYlaCFEbXEjai5IzWHJJ0na2tyYqyJ2o4Dq0DalDZF1X7G3kti9LVmsSdVWQRC2EqG3K9YrE8y3tS5M2gK2Vlub1XA2JO7gOrYLccXWwMVO04kokUV9EErUQojYr1yt2H88lMS2bLYez2Zx6hlP5polbo4HGPs60Ca5DVLA7bUPq4Od6facPRfWQRH0RSdRCiNuJUorDpwvZnJbNlrRsEtPOkHrq8scu13VzoG3I+cQdXIcGXk5otVc+3SiqXq19MpkQQoir02g0BHs6EuzpSP+oegCczCs2Ju3EtGz+OZ7DsbPnmL/9GPO3HwPATWdDVFAd2gS70yakDk39XeUCNQshiVoIIWo5L2c7ekb60TPSD4D84jKS0s8aW93b089ytrCUFSknWJFyAgA7ay0t6rlxR30POoR60qKeGzZWkrjNQRK1EELcZpzsrOkQ6kmHUE8ASsv1/HM8l8TU7PPnus+QXVDCptRsNqVm88nK/TjaWtGuvgcdGhrWC/V2+tc7c0TVkkQthBC3ORsrQ+u5RT03nu1UH6UUB08WkJiWTcKBU6w/eJrsghL+2pPFX3uyAEMrvUNDTzo09CSmoSe+rvZm/hS1lyRqIYQQJjQaDQ29nWjo7cRjbQPR6xUpmbkkHDjF2v2n2Jyazcm8YpNz3A29nYyJu139Ojjby+1gVUWu+hZCCFEpRaXlbEs/Q8KBU6zbf4qdx3K4OJNYaTW0qOdGzPnE3TJQzm9fSm7PuogkaiGEqF45haVsOHSKdecTd9rpQpPlF85vX0jcjXzk/LbcniWEEOKWcdXZ0KOpHz2aGq4qP5JdyPqDp1h34DQJB05d8fx2TAMPWge50yzAjTA/Z+ys5ZGn/0Za1EIIIarNxee31x04zebU0xSV6k3K2FppCfdzplmAG83rudE8wJX6Xk5Y1eIHsEjX90UkUQshhOUoLitn6+EzbDx4mh1Hc9hx1HAP96Ucba2IDHCl+fnk3SzAlbpuDrWmy1y6voUQQlgkO2sr2jfwpH0Dwz3cSimOZJ9jx9Gz7Dhylp1Hc0g+lkNBSTkbD2Wz8VC2cV0PR1tj0m4eYPjp4WRnro9yy0iiFkIIYTYajYZADx2BHjp6N/cHoKxcz4GT+ew8kkPS0bPsPHqWPRl5nL7kXDdAgLvD+Va3K80C3Iis64qjXe1KbbXr0wghhKjxrK20hPm6EObrQv82hueVF5WWszsjl53nW91JR89y6GQBR8+c4+iZcyxKzgAMI4U18HIi3M+FMF9nwv2cCfdzwdfFvsZ2m0uiFkIIYfHsbaxoFehOq0B347zcolKSz5/n3nnE8DMjp4gDWfkcyMrnjx0V67s62JxP3C7Gn418nHGwtfyrzSVRCyGEqJFc7G2IOf8I0wuycov4JyOXPRl5pGTksiczl4MnC8g5V2p8dvkFGg2EeDgS5udMmK+LMYkHuFvWRWuSqIUQQtQa3i72eLvYc1djb+O84rJyDmTlX5S8DT9PF5Rw6FQBh04VsDg501je2c6axr7OhPldaIG70NjXGScznfuWRC2EEKJWs7O2oom/K038XU3mn8wrZk9mriF5Z+SRkpnHgaw88orL2HL4DFsOnzEpH1hHR+sgdz5+pMUtjF4StRBCiNuUl7MdXs5edAz1Ms4rLddz6GQBKRm5pGQaEviezFxO5BaTnl2Ih5PtLY9TErUQQghxno2Vlsa+zjT2daYPdY3zswtK2JOZi15/lZWriSRqIYQQ4hrqONoaH9Jyq8m4Y0IIIYQFk0QthBBCWDBJ1EIIIYQFk0QthBBCWDBJ1EIIIYQFq/VXfevPX0ufkZFh5kiEEEIIgws5SX8d93vV+kR94sQJANq2bWvmSIQQQghTJ06cIDAw8KplNEopdYviMYuysjK2b9+Oj48PWu3N9fTn5eURERHB7t27cXZ2rqIIazeps8qTOqs8qbPKkzqrvKqsM71ez4kTJ2jZsiXW1ldvM9f6RF2VcnNzcXV1JScnBxcXF3OHUyNInVWe1FnlSZ1VntRZ5ZmrzuRiMiGEEMKCSaIWQgghLJgk6kqws7Pjrbfews7Oztyh1BhSZ5UndVZ5UmeVJ3VWeeaqMzlHLYQQQlgwaVELIYQQFkwStRBCCGHBJFELIYQQFkwSdSV89tlnBAcHY29vT7t27di8ebO5Q7JYEydOpE2bNjg7O+Pt7U2fPn3Yu3evucOqMd5//300Gg0jR440dygW7dixY/znP//Bw8MDBwcHIiMj2bJli7nDsljl5eW8+eabhISE4ODgQIMGDXj77beRS5VMrVmzht69e+Pv749Go2HBggUmy5VSjB07Fj8/PxwcHOjatSv79++vtngkUV+nH3/8kVGjRvHWW2+xbds2mjdvTvfu3cnKyjJ3aBZp9erVDBkyhI0bNxIfH09paSndunWjoKDA3KFZvMTERL744guaNWtm7lAs2pkzZ4iJicHGxoYlS5awe/duPvroI9zd3c0dmsX64IMPmDFjBp9++ikpKSl88MEHfPjhh0yfPt3coVmUgoICmjdvzmeffXbF5R9++CHTpk1j5syZbNq0CUdHR7p3705RUVH1BKTEdWnbtq0aMmSI8X15ebny9/dXEydONGNUNUdWVpYC1OrVq80dikXLy8tToaGhKj4+XnXu3FmNGDHC3CFZrDFjxqgOHTqYO4wapVevXuqpp54ymffggw+q2NhYM0Vk+QA1f/5843u9Xq98fX3VpEmTjPPOnj2r7Ozs1A8//FAtMUiL+jqUlJSwdetWunbtapyn1Wrp2rUrGzZsMGNkNUdOTg4AderUMXMklm3IkCH06tXL5G9NXNnChQuJiori4Ycfxtvbm5YtW/Lll1+aOyyL1r59e1auXMm+ffsA2LFjB+vWraNnz55mjqzmSE1NJTMz0+R/1NXVlXbt2lVbPqj1o2dVhVOnTlFeXo6Pj4/JfB8fH/bs2WOmqGoOvV7PyJEjiYmJoWnTpuYOx2LNmzePbdu2kZiYaO5QaoRDhw4xY8YMRo0axX//+18SExMZPnw4tra2xMXFmTs8i/Tqq6+Sm5tLWFgYVlZWlJeX8+677xIbG2vu0GqMzMxMgCvmgwvLqpokalHthgwZwq5du1i3bp25Q7FYR44cYcSIEcTHx2Nvb2/ucGoEvV5PVFQU7733HgAtW7Zk165dzJw5UxL1v/jpp5/4/vvvmTt3Lk2aNCEpKYmRI0fi7+8vdWbBpOv7Onh6emJlZWUc2/qCEydO4Ovra6aoaoahQ4fy559/smrVKgICAswdjsXaunUrWVlZtGrVCmtra6ytrVm9ejXTpk3D2tqa8vJyc4docfz8/IiIiDCZFx4eTnp6upkisnwvv/wyr776Ko8++iiRkZEMGDCAF198kYkTJ5o7tBrjwnf+rcwHkqivg62tLa1bt2blypXGeXq9npUrVxIdHW3GyCyXUoqhQ4cyf/58/vrrL0JCQswdkkXr0qULycnJJCUlGaeoqChiY2NJSkrCysrK3CFanJiYmMtu+du3bx9BQUFmisjyFRYWotWafu1bWVmh1+vNFFHNExISgq+vr0k+yM3NZdOmTdWWD6Tr+zqNGjWKuLg4oqKiaNu2LVOnTqWgoIAnn3zS3KFZpCFDhjB37lx+//13nJ2djeduXF1dcXBwMHN0lsfZ2fmy8/eOjo54eHjIef1/8eKLL9K+fXvee+89+vfvz+bNm5k1axazZs0yd2gWq3fv3rz77rsEBgbSpEkTtm/fzpQpU3jqqafMHZpFyc/P58CBA8b3qampJCUlUadOHQIDAxk5ciTvvPMOoaGhhISE8Oabb+Lv70+fPn2qJ6BquZa8lpo+fboKDAxUtra2qm3btmrjxo3mDsliAVecZs+ebe7Qagy5Peva/vjjD9W0aVNlZ2enwsLC1KxZs8wdkkXLzc1VI0aMUIGBgcre3l7Vr19fvf7666q4uNjcoVmUVatWXfH7Ky4uTilluEXrzTffVD4+PsrOzk516dJF7d27t9rikdGzhBBCCAsm56iFEEIICyaJWgghhLBgkqiFEEIICyaJWgghhLBgkqiFEEIICyaJWgghhLBgkqiFEEIICyaJWgghhLBgkqiFEFVOo9GwYMECc4chRK0giVqIWmbgwIFoNJrLph49epg7NCHEDZBBOYSohXr06MHs2bNN5tnZ2ZkpGiHEzZAWtRC1kJ2dHb6+viaTu7s7YOiWnjFjBj179sTBwYH69evzyy+/mKyfnJzM3XffjYODAx4eHgwaNIj8/HyTMl9//TVNmjTBzs4OPz8/hg4darL81KlT9O3bF51OR2hoKAsXLjQuO3PmDLGxsXh5eeHg4EBoaOhlBxZCCANJ1ELcht5880369evHjh07iI2N5dFHHyUlJQWAgoICunfvjru7O4mJifz888+sWLHCJBHPmDGDIUOGMGjQIJKTk1m4cCENGzY02cf48ePp378/O3fu5N577yU2Npbs7Gzj/nfv3s2SJUtISUlhxowZeHp63roKEKImqbZxuYQQZhEXF6esrKyUo6OjyfTuu+8qpQxDkD7//PMm67Rr10698MILSimlZs2apdzd3VV+fr5x+aJFi5RWq1WZmZlKKaX8/f3V66+//q8xAOqNN94wvs/Pz1eAWrJkiVJKqd69e6snn3yyaj6wELWcnKMWoha66667mDFjhsm8OnXqGF9HR0ebLIuOjiYpKQmAlJQUmjdvjqOjo3F5TEwMer2evXv3otFoOH78OF26dLlqDM2aNTO+dnR0xMXFhaysLABeeOEF+vXrx7Zt2+jWrRt9+vShffv2N/RZhajtJFELUQs5Ojpe1hVdVRwcHK6rnI2Njcl7jUaDXq8HoGfPnhw+fJjFixcTHx9Ply5dGDJkCJMnT67yeIWo6eQctRC3oY0bN172Pjw8HIDw8HB27NhBQUGBcXlCQgJarZbGjRvj7OxMcHAwK1euvKkYvLy8iIuL47vvvmPq1KnMmjXrprYnRG0lLWohaqHi4mIyMzNN5llbWxsv2Pr555+JioqiQ4cOfP/992zevJn/+7//AyA2Npa33nqLuLg4xo0bx8mTJxk2bBgDBgzAx8cHgHHjxvH888/j7e1Nz549ycvLIyEhgWHDhl1XfGPHjqV169Y0adKE4uJi/vzzT+OBghDClCRqIWqhpUuX4ufnZzKvcePG7NmzBzBckT1v3jwGDx6Mn58fP/zwAxEREQDodDqWLVvGiBEjaNOmDTqdjn79+jFlyhTjtuLi4igqKuLjjz9m9OjReHp68tBDD113fLa2trz22mukpaXh4OBAx44dmTdvXhV8ciFqH41SSpk7CCHEraPRaJg/fz59+vQxdyhCiOsg56iFEEIICyaJWgghhLBgco5aiNuMnO0SomaRFrUQQghhwSRRCyGEEBZMErUQQghhwSRRCyGEEBZMErUQQghhwSRRCyGEEBZMErUQQghhwSRRCyGEEBZMErUQQghhwf4fz5L3r+0fe0QAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax2 = ax1.twiny()  # creates a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Decoding strategies to control randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=cfg[\"context_length\"]\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 Temperature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2 Top-k sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demonstration purposes\n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_logits=tensor([6.7500, 6.2800, 4.5100])\n",
      "top_pos=tensor([3, 7, 0])\n",
      "new_logits=tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n",
      "topk_probas=tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "print(f\"{top_logits=}\")\n",
    "print(f\"{top_pos=}\")\n",
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1],\n",
    "    input=torch.tensor(float('-inf')),\n",
    "    other=next_token_logits\n",
    ")\n",
    "print(f\"{new_logits=}\")\n",
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(f\"{topk_probas=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.3 Modifying the text generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "\n",
    "def generate(model, idx, max_new_tokens: int, context_size: int, temperature=0.0, top_k: Optional[int] = None, eos_id=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        # Get last token in current sequence\n",
    "        logits = logits[:, -1, :]\n",
    "        # top-k sampling\n",
    "        if top_k is not None:\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(\n",
    "                condition=logits < min_val,\n",
    "                input=torch.tensor(float('-inf')).to(logits.device),\n",
    "                other=logits\n",
    "            )\n",
    "        if temperature > 0.0:\n",
    "            # temperature scaling\n",
    "            logits = logits / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            # greedy decoding\n",
    "            idx_next = torch.argmax(probs, dim=-1, keepdim=True)\n",
    "        # check if we've reached the end\n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "        # append generated token to current sequence for further generation\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you know began to happen a little wild--I was such a good; and\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=cfg[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Loading and saving model weights in PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26173/2634080021.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"model.pth\", map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPTModel(\n",
    "    vocab_size=cfg[\"vocab_size\"],\n",
    "    context_length=cfg[\"context_length\"],\n",
    "    drop_rate=cfg[\"drop_rate\"],\n",
    "    emb_dim=cfg[\"emb_dim\"],\n",
    "    n_heads=cfg[\"n_heads\"],\n",
    "    n_layers=cfg[\"n_layers\"],\n",
    "    qkv_bias=cfg[\"qkv_bias\"]\n",
    ")\n",
    "model.load_state_dict(torch.load(\"model.pth\", map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    },\n",
    "    \"model_and_optimizer.pth\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26173/1266802268.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"model_and_optimizer.pth\", map_location=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"model_and_optimizer.pth\", map_location=device)\n",
    "model = GPTModel(\n",
    "    vocab_size=cfg[\"vocab_size\"],\n",
    "    context_length=cfg[\"context_length\"],\n",
    "    drop_rate=cfg[\"drop_rate\"],\n",
    "    emb_dim=cfg[\"emb_dim\"],\n",
    "    n_heads=cfg[\"n_heads\"],\n",
    "    n_layers=cfg[\"n_layers\"],\n",
    "    qkv_bias=cfg[\"qkv_bias\"]\n",
    ")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Loading pretrained weights from OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt_download.py', <http.client.HTTPMessage at 0x7f6f25e6f260>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/\"\n",
    "    \"LLMs-from-scratch/main/ch05/\"\n",
    "    \"01_main-chapter-code/gpt_download.py\"\n",
    ")\n",
    "filename = url.split('/')[-1]\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-07 17:41:39.086042: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1736268099.131932   26173 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1736268099.147918   26173 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-07 17:41:39.277881: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-07 17:41:48.355160: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 154389504 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "from build_a_large_language_model_from_scratch.gpt_download import download_and_load_gpt2\n",
    "\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=\"124M\", models_dir=\"gpt2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Settings:\", settings)\n",
    "print(\"Parameter dictionary keys:\", params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
      "   0.04531523]\n",
      " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
      "   0.04318958]\n",
      " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
      "  -0.08785918]\n",
      " ...\n",
      " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
      "  -0.06952604]\n",
      " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
      "  -0.02245961]\n",
      " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
      "   0.12067825]]\n",
      "Token embeddign weight tensor dimensions: (50257, 768)\n"
     ]
    }
   ],
   "source": [
    "print(params[\"wte\"])\n",
    "print(\"Token embeddign weight tensor dimensions:\", params[\"wte\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2-small (124M)\"\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt = GPTModel(\n",
    "    context_length=NEW_CONFIG[\"context_length\"],\n",
    "    drop_rate=NEW_CONFIG[\"drop_rate\"],\n",
    "    emb_dim=NEW_CONFIG[\"emb_dim\"],\n",
    "    n_heads=NEW_CONFIG[\"n_heads\"],\n",
    "    n_layers=NEW_CONFIG[\"n_layers\"],\n",
    "    qkv_bias=NEW_CONFIG[\"qkv_bias\"],\n",
    "    vocab_size=NEW_CONFIG[\"vocab_size\"]\n",
    ")\n",
    "\n",
    "gpt.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    \"\"\"Assigns values from right tensor to left tensor after shape validation.\n",
    "    \n",
    "    Args:\n",
    "        left: Target PyTorch tensor/parameter\n",
    "        right: Source tensor/array to copy values from\n",
    "        \n",
    "    Returns:\n",
    "        torch.nn.Parameter: New parameter containing values from right tensor\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If shapes of left and right tensors don't match\n",
    "    \"\"\"\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params[\"wpe\"])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params[\"wte\"])\n",
    "\n",
    "    # iterative over transformer blocks\n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        # split is used to divide attention and bias weights into three equal parts for the qkv components\n",
    "        # load attention qkv weights\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1\n",
    "        )\n",
    "        gpt.trf_blocks[b].layers[0][1].W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].layers[0][1].W_query.weight, q_w.T\n",
    "        )\n",
    "        gpt.trf_blocks[b].layers[0][1].W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].layers[0][1].W_key.weight, k_w.T\n",
    "        )\n",
    "        gpt.trf_blocks[b].layers[0][1].W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].layers[0][1].W_value.weight, v_w.T\n",
    "        )\n",
    "\n",
    "        # load attn qkv bias\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1\n",
    "        )\n",
    "        gpt.trf_blocks[b].layers[0][1].W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].layers[0][1].W_query.bias, q_b\n",
    "        )\n",
    "        gpt.trf_blocks[b].layers[0][1].W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].layers[0][1].W_key.bias, k_b\n",
    "        )\n",
    "        gpt.trf_blocks[b].layers[0][1].W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].layers[0][1].W_value.bias, v_b\n",
    "        )\n",
    "\n",
    "        # load attn linear projection weights\n",
    "        gpt.trf_blocks[b].layers[0][1].out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].layers[0][1].out_proj.weight,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T,\n",
    "        )\n",
    "        gpt.trf_blocks[b].layers[0][1].out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].layers[0][1].out_proj.bias,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"],\n",
    "        )\n",
    "\n",
    "        # load feedforward network weights and biases\n",
    "        gpt.trf_blocks[b].layers[1][1].layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].layers[1][1].layers[0].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T,\n",
    "        )\n",
    "        gpt.trf_blocks[b].layers[1][1].layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].layers[1][1].layers[0].bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"]\n",
    "        )\n",
    "        gpt.trf_blocks[b].layers[1][1].layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].layers[1][1].layers[2].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T,\n",
    "        )\n",
    "        gpt.trf_blocks[b].layers[1][1].layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].layers[1][1].layers[2].bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"],\n",
    "        )\n",
    "\n",
    "        # load layer norm params\n",
    "        gpt.trf_blocks[b].layers[0][0].scale = assign(\n",
    "            gpt.trf_blocks[b].layers[0][0].scale,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"]\n",
    "        )\n",
    "        gpt.trf_blocks[b].layers[0][0].shift = assign(\n",
    "            gpt.trf_blocks[b].layers[0][0].shift,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"]\n",
    "        )\n",
    "        gpt.trf_blocks[b].layers[1][0].scale = assign(\n",
    "            gpt.trf_blocks[b].layers[1][0].scale,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"]\n",
    "        )\n",
    "        gpt.trf_blocks[b].layers[1][0].shift = assign(\n",
    "            gpt.trf_blocks[b].layers[1][0].shift,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"]\n",
    "        )\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    # Original GPT-2 model reused the token embedding weights to reduce the total number of params (weight tying)\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): MultiHeadAttention(\n",
       "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you as far as the hand can go until the end of your turn unless something interrupts your control flow. As you may observe I\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.5\n",
    "Calculate the training and validation set losses of the GPTModel with the pretrained weights from OpenAI on the The Verdict dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.7547627290089927, 3.5596330165863037)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss, val_loss = evaluate_model(gpt, train_loader, val_loader, device, 10)\n",
    "\n",
    "train_loss, val_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
